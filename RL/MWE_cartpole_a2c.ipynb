{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coax\n",
    "import gym\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from coax.value_losses import mse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the name of this script\n",
    "name = 'a2c'\n",
    "\n",
    "# the cart-pole MDP\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make(\"rpp_gym:InclinedCartpole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = coax.wrappers.TrainMonitor(env, name=name, tensorboard_dir=f\"./data/tensorboard/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.spec.max_episode_steps = 200\n",
    "# env.spec.reward_threshold = 195.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlp import T, Scalar\n",
    "from emlp.groups import SO, S, O, Trivial,Z\n",
    "import emlp.nn.haiku as ehk\n",
    "from emlp.reps import Rep\n",
    "from emlp.nn import gated,gate_indices,uniform_rep\n",
    "from math import prod\n",
    "from representations import PseudoScalar\n",
    "from mixed_emlp_haiku import MixedEMLP\n",
    "\n",
    "## Trivial\n",
    "# group=Trivial(2)\n",
    "# rep_in = T(0)*prod(env.observation_space.shape)\n",
    "# rep_out = T(0)*env.action_space.n#prod(env.action_space.shape)\n",
    "\n",
    "## Reflection\n",
    "group=Z(2)\n",
    "rep_in = PseudoScalar()*prod(env.observation_space.shape)\n",
    "rep_out = T(1)#*env.action_space.n#prod(env.action_space.shape)\n",
    "\n",
    "nn_pi = ehk.EMLP(rep_in,rep_out,group,ch=100,num_layers=2)\n",
    "nn_v = ehk.EMLP(rep_in,T(0),group,ch=100,num_layers=2)\n",
    "\n",
    "# nn_pi = ehk.MLP(rep_in,rep_out(group),group,ch=100,num_layers=2)\n",
    "# nn_v = ehk.MLP(rep_in,T(0),group,ch=100,num_layers=2)\n",
    "\n",
    "nn_pi = MixedEMLP(rep_in,rep_out(group),group,ch=100,num_layers=2)\n",
    "nn_v = MixedEMLP(rep_in,T(0),group,ch=100,num_layers=2)\n",
    "\n",
    "\n",
    "def func_pi(S, is_training):\n",
    "    return {'logits': nn_pi(S)}\n",
    "\n",
    "\n",
    "def func_v(S, is_training):\n",
    "    return nn_v(S).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "# def func_pi(S, is_training):\n",
    "#     logits = hk.Sequential((\n",
    "#         hk.Linear(16), jax.nn.relu,\n",
    "#         hk.Linear(16), jax.nn.relu,\n",
    "#         hk.Linear(16), jax.nn.relu,\n",
    "#         hk.Linear(env.action_space.n, w_init=jnp.zeros)\n",
    "#     ))\n",
    "#     return {'logits': logits(S)}\n",
    "\n",
    "\n",
    "\n",
    "# def func_v(S, is_training):\n",
    "#     value = hk.Sequential((\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(1, w_init=jnp.zeros), jnp.ravel\n",
    "#     ))\n",
    "#     return value(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# these optimizers collect batches of grads before applying updates\n",
    "optimizer_v = optax.chain(optax.apply_every(k=32), optax.adam(0.002))\n",
    "optimizer_pi = optax.chain(optax.apply_every(k=32), optax.adam(0.001))\n",
    "\n",
    "\n",
    "# value function and its derived policy\n",
    "v = coax.V(func_v, env)\n",
    "pi = coax.Policy(func_pi, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = v.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experience tracer\n",
    "tracer = coax.reward_tracing.NStep(n=1, gamma=0.9)\n",
    "\n",
    "# updaters\n",
    "vanilla_pg = coax.policy_objectives.VanillaPG(pi, optimizer=optimizer_pi)\n",
    "simple_td = coax.td_learning.SimpleTD(v, loss_function=mse, optimizer=optimizer_v)\n",
    "\n",
    "epoch_rewards = []\n",
    "\n",
    "# train\n",
    "for ep in range(1000):\n",
    "    s = env.reset()\n",
    "    er = 0\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        a = pi(s)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "        \n",
    "        if done and (t == env.spec.max_episode_steps - 1):\n",
    "            r = 1 / (1 - tracer.gamma)\n",
    "        er+=r\n",
    "        tracer.add(s, a, r, done)\n",
    "        while tracer:\n",
    "            transition_batch = tracer.pop()\n",
    "            metrics_v, td_error = simple_td.update(transition_batch, return_td_error=True)\n",
    "            metrics_pi = vanilla_pg.update(transition_batch, td_error)\n",
    "            env.record_metrics(metrics_v)\n",
    "            env.record_metrics(metrics_pi)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        s = s_next\n",
    "    \n",
    "    print(\"Epoch reward\",er)\n",
    "    epoch_rewards.append(er)\n",
    "    # early stopping\n",
    "    if env.avg_G > env.spec.reward_threshold:\n",
    "        break\n",
    "\n",
    "\n",
    "# run env one more time to render\n",
    "#coax.utils.generate_gif(env, policy=pi, filepath=f\"./data/{name}.gif\", duration=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coax.utils.dump(pi.params, \"./emlp_pi_params.lz4\")\n",
    "coax.utils.dump(v.params, \"./emlp_v_params.lz4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
