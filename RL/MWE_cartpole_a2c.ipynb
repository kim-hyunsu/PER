{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coax\n",
    "import gym\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from coax.value_losses import mse\n",
    "\n",
    "\n",
    "\n",
    "# the name of this script\n",
    "name = 'a2c'\n",
    "\n",
    "# the cart-pole MDP\n",
    "env = gym.make('CartPole-v0')\n",
    "env = coax.wrappers.TrainMonitor(env, name=name, tensorboard_dir=f\"./data/tensorboard/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Baseline MLPS\n",
    "\n",
    "# def func_pi(S, is_training):\n",
    "#     logits = hk.Sequential((\n",
    "#         hk.Linear(8), jax.nn.relu,\n",
    "#         hk.Linear(8), jax.nn.relu,\n",
    "#         hk.Linear(8), jax.nn.relu,\n",
    "#         hk.Linear(env.action_space.n, w_init=jnp.zeros)\n",
    "#     ))\n",
    "#     return {'logits': logits(S)}\n",
    "\n",
    "\n",
    "# def func_v(S, is_training):\n",
    "#     value = hk.Sequential((\n",
    "#         hk.Linear(8), jax.nn.relu,\n",
    "#         hk.Linear(8), jax.nn.relu,\n",
    "#         hk.Linear(8), jax.nn.relu,\n",
    "#         hk.Linear(1, w_init=jnp.zeros), jnp.ravel\n",
    "#     ))\n",
    "#     return value(S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|root|INFO] Initing EMLP\n",
      "[a2c|root|INFO] Linear W components:400 rep:96P+48P⊗V+20P⊗V²+8P⊗V³+4P⊗V⁴\n",
      "[a2c|root|INFO] Linear W components:10000 rep:576V⁰+576V+384V²+216V³+121V⁴+44V⁵+14V⁶+4V⁷+V⁸\n",
      "[a2c|root|INFO] Linear W components:200 rep:24V+12V²+5V³+2V⁴+V⁵\n",
      "[a2c|root|INFO] Initing EMLP\n",
      "[a2c|root|INFO] Linear W components:400 rep:96P+48P⊗V+20P⊗V²+8P⊗V³+4P⊗V⁴\n",
      "[a2c|root|INFO] Linear W components:10000 rep:576V⁰+576V+384V²+216V³+121V⁴+44V⁵+14V⁶+4V⁷+V⁸\n",
      "[a2c|root|INFO] Linear W components:100 rep:24V⁰+12V+5V²+2V³+V⁴\n"
     ]
    }
   ],
   "source": [
    "from emlp import T, Scalar\n",
    "from emlp.groups import SO, S, O, Trivial,Z\n",
    "from emlp_haiku import EMLPBlock, Sequential, Linear,EMLP\n",
    "from emlp.reps import Rep\n",
    "from emlp.nn import gated,gate_indices,uniform_rep\n",
    "from math import prod\n",
    "from representations import PseudoScalar\n",
    "\n",
    "## Trivial\n",
    "# group=Trivial(2)\n",
    "# rep_in = T(0)*prod(env.observation_space.shape)\n",
    "# rep_out = T(0)*env.action_space.n#prod(env.action_space.shape)\n",
    "\n",
    "## Reflection\n",
    "group=Z(2)\n",
    "rep_in = PseudoScalar()*prod(env.observation_space.shape)\n",
    "rep_out = T(1)#*env.action_space.n#prod(env.action_space.shape)\n",
    "\n",
    "nn_pi = EMLP(rep_in,rep_out,group,ch=100,num_layers=2)\n",
    "nn_v = EMLP(rep_in,T(0),group,ch=100,num_layers=2)\n",
    "\n",
    "def func_pi(S, is_training):\n",
    "    return {'logits': nn_pi(S)}\n",
    "\n",
    "\n",
    "def func_v(S, is_training):\n",
    "    return nn_v(S).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 1,\tT: 13,\tG: 12,\tavg_r: 1,\tavg_G: 12,\tt: 12,\tdt: 666.033ms,\tSimpleTD/loss: 0.612,\tVanillaPG/loss: 0.758\n",
      "[a2c|TrainMonitor|INFO] ep: 2,\tT: 26,\tG: 12,\tavg_r: 1,\tavg_G: 12,\tt: 12,\tdt: 15.533ms,\tSimpleTD/loss: 0.612,\tVanillaPG/loss: 0.756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 12.0\n",
      "Epoch reward 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 3,\tT: 42,\tG: 15,\tavg_r: 1,\tavg_G: 13,\tt: 15,\tdt: 14.677ms,\tSimpleTD/loss: 0.529,\tVanillaPG/loss: 0.718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 4,\tT: 67,\tG: 24,\tavg_r: 1,\tavg_G: 15.8,\tt: 24,\tdt: 14.339ms,\tSimpleTD/loss: 0.426,\tVanillaPG/loss: 0.611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 5,\tT: 186,\tG: 118,\tavg_r: 1,\tavg_G: 36.2,\tt: 118,\tdt: 14.087ms,\tSimpleTD/loss: 0.602,\tVanillaPG/loss: 0.258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 118.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 6,\tT: 207,\tG: 20,\tavg_r: 1,\tavg_G: 33.5,\tt: 20,\tdt: 14.587ms,\tSimpleTD/loss: 1.37,\tVanillaPG/loss: -0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 7,\tT: 283,\tG: 75,\tavg_r: 1,\tavg_G: 39.4,\tt: 75,\tdt: 14.075ms,\tSimpleTD/loss: 0.406,\tVanillaPG/loss: 0.194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 75.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 8,\tT: 352,\tG: 68,\tavg_r: 1,\tavg_G: 43,\tt: 68,\tdt: 13.971ms,\tSimpleTD/loss: 0.587,\tVanillaPG/loss: 0.028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 68.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 9,\tT: 472,\tG: 119,\tavg_r: 1,\tavg_G: 51.4,\tt: 119,\tdt: 13.906ms,\tSimpleTD/loss: 0.646,\tVanillaPG/loss: -0.0497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 119.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 10,\tT: 507,\tG: 34,\tavg_r: 1,\tavg_G: 49.7,\tt: 34,\tdt: 13.985ms,\tSimpleTD/loss: 0.398,\tVanillaPG/loss: 0.0163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 11,\tT: 648,\tG: 140,\tavg_r: 1,\tavg_G: 58.7,\tt: 140,\tdt: 13.991ms,\tSimpleTD/loss: 0.0747,\tVanillaPG/loss: 0.0924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 140.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 12,\tT: 787,\tG: 138,\tavg_r: 1,\tavg_G: 66.7,\tt: 138,\tdt: 13.608ms,\tSimpleTD/loss: 0.311,\tVanillaPG/loss: -0.0803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 138.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 13,\tT: 879,\tG: 91,\tavg_r: 1,\tavg_G: 69.1,\tt: 91,\tdt: 14.788ms,\tSimpleTD/loss: 0.355,\tVanillaPG/loss: -0.144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 91.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 14,\tT: 1,080,\tG: 200,\tavg_r: 1,\tavg_G: 82.2,\tt: 200,\tdt: 13.738ms,\tSimpleTD/loss: 0.00657,\tVanillaPG/loss: 0.00404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 15,\tT: 1,199,\tG: 118,\tavg_r: 1,\tavg_G: 85.8,\tt: 118,\tdt: 13.916ms,\tSimpleTD/loss: 0.221,\tVanillaPG/loss: -0.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 118.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 16,\tT: 1,321,\tG: 121,\tavg_r: 1,\tavg_G: 89.3,\tt: 121,\tdt: 14.869ms,\tSimpleTD/loss: 0.242,\tVanillaPG/loss: -0.044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 121.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 17,\tT: 1,473,\tG: 151,\tavg_r: 1,\tavg_G: 95.5,\tt: 151,\tdt: 13.777ms,\tSimpleTD/loss: 0.127,\tVanillaPG/loss: 0.0508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 151.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 18,\tT: 1,628,\tG: 154,\tavg_r: 1,\tavg_G: 101,\tt: 154,\tdt: 14.604ms,\tSimpleTD/loss: 0.129,\tVanillaPG/loss: -0.00902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 154.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 19,\tT: 1,798,\tG: 169,\tavg_r: 1,\tavg_G: 108,\tt: 169,\tdt: 14.025ms,\tSimpleTD/loss: 0.0353,\tVanillaPG/loss: 0.0285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 169.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 20,\tT: 1,999,\tG: 200,\tavg_r: 1,\tavg_G: 117,\tt: 200,\tdt: 13.824ms,\tSimpleTD/loss: 0.00728,\tVanillaPG/loss: -0.0111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 21,\tT: 2,200,\tG: 200,\tavg_r: 1,\tavg_G: 126,\tt: 200,\tdt: 13.666ms,\tSimpleTD/loss: 5.69e-05,\tVanillaPG/loss: -0.000748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 22,\tT: 2,401,\tG: 200,\tavg_r: 1,\tavg_G: 133,\tt: 200,\tdt: 13.909ms,\tSimpleTD/loss: 0.000882,\tVanillaPG/loss: 0.00169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 23,\tT: 2,602,\tG: 200,\tavg_r: 1,\tavg_G: 140,\tt: 200,\tdt: 13.988ms,\tSimpleTD/loss: 0.00176,\tVanillaPG/loss: -0.00571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 24,\tT: 2,803,\tG: 200,\tavg_r: 1,\tavg_G: 146,\tt: 200,\tdt: 14.095ms,\tSimpleTD/loss: 5.86e-05,\tVanillaPG/loss: -0.00213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 25,\tT: 2,988,\tG: 184,\tavg_r: 1,\tavg_G: 150,\tt: 184,\tdt: 13.803ms,\tSimpleTD/loss: 0.205,\tVanillaPG/loss: -0.00664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 184.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# these optimizers collect batches of grads before applying updates\n",
    "optimizer_v = optax.chain(optax.apply_every(k=32), optax.adam(0.002))\n",
    "optimizer_pi = optax.chain(optax.apply_every(k=32), optax.adam(0.001))\n",
    "\n",
    "\n",
    "# value function and its derived policy\n",
    "v = coax.V(func_v, env)\n",
    "pi = coax.Policy(func_pi, env)\n",
    "\n",
    "# experience tracer\n",
    "tracer = coax.reward_tracing.NStep(n=1, gamma=0.9)\n",
    "\n",
    "# updaters\n",
    "vanilla_pg = coax.policy_objectives.VanillaPG(pi, optimizer=optimizer_pi)\n",
    "simple_td = coax.td_learning.SimpleTD(v, loss_function=mse, optimizer=optimizer_v)\n",
    "\n",
    "\n",
    "# train\n",
    "for ep in range(1000):\n",
    "    s = env.reset()\n",
    "    er = 0\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        a = pi(s)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "        \n",
    "        if done and (t == env.spec.max_episode_steps - 1):\n",
    "            r = 1 / (1 - tracer.gamma)\n",
    "        er+=r\n",
    "        tracer.add(s, a, r, done)\n",
    "        while tracer:\n",
    "            transition_batch = tracer.pop()\n",
    "            metrics_v, td_error = simple_td.update(transition_batch, return_td_error=True)\n",
    "            metrics_pi = vanilla_pg.update(transition_batch, td_error)\n",
    "            env.record_metrics(metrics_v)\n",
    "            env.record_metrics(metrics_pi)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        s = s_next\n",
    "    \n",
    "    print(\"Epoch reward\",er)\n",
    "    # early stopping\n",
    "    if env.avg_G > env.spec.reward_threshold:\n",
    "        break\n",
    "\n",
    "\n",
    "# run env one more time to render\n",
    "#coax.utils.generate_gif(env, policy=pi, filepath=f\"./data/{name}.gif\", duration=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
