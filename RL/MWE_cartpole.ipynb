{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f572bc44-883a-4690-bf6c-da2048ea95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coax\n",
    "import gym\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from coax.value_losses import mse\n",
    "from optax import adam\n",
    "\n",
    "from emlp import T, Scalar\n",
    "from emlp.groups import SO, S, O, Trivial\n",
    "from emlp_haiku import EMLPBlock, Sequential, Linear\n",
    "from emlp.reps import Rep\n",
    "from emlp.nn import gated,gate_indices,uniform_rep\n",
    "\n",
    "# the name of this script\n",
    "name = 'dqn'\n",
    "\n",
    "# the cart-pole MDP\n",
    "env = gym.make('CartPole-v1')\n",
    "env = coax.wrappers.TrainMonitor(env, name=name, tensorboard_dir=f\"./data/tensorboard/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f63058-6c62-4e33-8c55-037f66c1af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch=300\n",
    "num_layers=4\n",
    "group = Trivial(2)\n",
    "rep_in = 4*T(0)(group)\n",
    "rep_out = 2*T(0)(group)\n",
    "# Parse ch as a single int, a sequence of ints, a single Rep, a sequence of Reps\n",
    "if isinstance(ch,int): middle_layers = num_layers*[uniform_rep(ch,group)]\n",
    "elif isinstance(ch,Rep): middle_layers = num_layers*[ch(group)]\n",
    "else: middle_layers = [(c(group) if isinstance(c,Rep) else uniform_rep(c,group)) for c in ch]\n",
    "# assert all((not rep.G is None) for rep in middle_layers[0].reps)\n",
    "reps = [rep_in]+middle_layers\n",
    "\n",
    "\n",
    "norms = 100000*jnp.array([1., 1., 1., 1.])\n",
    "\n",
    "def func(S, is_training):\n",
    "    network = Sequential(\n",
    "        *[EMLPBlock(rin,rout) for rin,rout in zip(reps,reps[1:])],\n",
    "        Linear(reps[-1],rep_out)\n",
    "    )\n",
    "    \n",
    "    return network(S/norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d6be43-8a1e-48a1-9dcc-be60a9a9481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def func(S, is_training):\n",
    "#     \"\"\" type-2 q-function: s -> q(s,.) \"\"\"\n",
    "#     seq = hk.Sequential((\n",
    "#         hk.Linear(8), jax.nn.relu,\n",
    "#         hk.Linear(8), jax.nn.relu,\n",
    "#         hk.Linear(8), jax.nn.relu,\n",
    "#         hk.Linear(env.action_space.n, w_init=jnp.zeros)\n",
    "#     ))\n",
    "#     return seq(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f637ebc0-8d6a-494b-832f-3ce7d45ad37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'dqn'\n",
    "\n",
    "# the cart-pole MDP\n",
    "env = gym.make('CartPole-v0')\n",
    "env = coax.wrappers.TrainMonitor(env, name=name, tensorboard_dir=f\"./data/tensorboard/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a444e822-38a1-4673-a651-f5a692a7911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value function and its derived policy\n",
    "q = coax.Q(func, env)\n",
    "pi = coax.BoltzmannPolicy(q, temperature=0.1)\n",
    "\n",
    "# target network\n",
    "q_targ = q.copy()\n",
    "\n",
    "# experience tracer\n",
    "tracer = coax.reward_tracing.NStep(n=1, gamma=0.9)\n",
    "buffer = coax.experience_replay.SimpleReplayBuffer(capacity=100000)\n",
    "\n",
    "# updater\n",
    "qlearning = coax.td_learning.QLearning(q, q_targ=q_targ, loss_function=mse, optimizer=adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e599d5d1-a74c-48da-b027-3008a711a5d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Reward = 10.0\n",
      "Epoch Reward = 9.0\n",
      "Epoch Reward = 8.0\n",
      "Epoch Reward = 10.0\n",
      "Epoch Reward = 10.0\n",
      "Epoch Reward = 9.0\n",
      "Epoch Reward = 9.0\n",
      "Epoch Reward = 10.0\n",
      "Epoch Reward = 9.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  101478610000000.0\n",
      "Loss =  1048561700000.0\n",
      "Grad Norm =  53500690000000.0\n",
      "Loss =  1531186800000.0\n",
      "Grad Norm =  4252040200000.0\n",
      "Loss =  151166780000.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  3144097700000.0\n",
      "Loss =  74999870000.0\n",
      "Grad Norm =  679595500000.0\n",
      "Loss =  30716199000.0\n",
      "Grad Norm =  4446692000000.0\n",
      "Loss =  38615990000.0\n",
      "Grad Norm =  5745571000000.0\n",
      "Loss =  73268330000.0\n",
      "Grad Norm =  660694240000.0\n",
      "Loss =  21297998000.0\n",
      "Grad Norm =  2590585400000.0\n",
      "Loss =  43725296000.0\n",
      "Grad Norm =  2548062000000.0\n",
      "Loss =  38923920000.0\n",
      "Grad Norm =  1610246700000.0\n",
      "Loss =  8724596000.0\n",
      "Grad Norm =  2630929000000.0\n",
      "Loss =  25313634000.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  1842807200000.0\n",
      "Loss =  21367595000.0\n",
      "Grad Norm =  564157600000.0\n",
      "Loss =  10168765000.0\n",
      "Grad Norm =  1616354000000.0\n",
      "Loss =  18693323000.0\n",
      "Grad Norm =  786935200000.0\n",
      "Loss =  18528473000.0\n",
      "Grad Norm =  715178000000.0\n",
      "Loss =  11620473000.0\n",
      "Grad Norm =  195377700000.0\n",
      "Loss =  4253246500.0\n",
      "Grad Norm =  1071531800000.0\n",
      "Loss =  13621991000.0\n",
      "Grad Norm =  1048229640000.0\n",
      "Loss =  10183572000.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  243055580000.0\n",
      "Loss =  5261513700.0\n",
      "Grad Norm =  16610723000.0\n",
      "Loss =  5909060600.0\n",
      "Grad Norm =  403158070000.0\n",
      "Loss =  5093069300.0\n",
      "Grad Norm =  438492270000.0\n",
      "Loss =  5634373600.0\n",
      "Grad Norm =  399895920000.0\n",
      "Loss =  4388412400.0\n",
      "Grad Norm =  252856370000.0\n",
      "Loss =  2499696600.0\n",
      "Grad Norm =  59731927000.0\n",
      "Loss =  3052031000.0\n",
      "Grad Norm =  113103176000.0\n",
      "Loss =  1401836800.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  256054170000.0\n",
      "Loss =  1361309600.0\n",
      "Grad Norm =  286542200000.0\n",
      "Loss =  2807650600.0\n",
      "Grad Norm =  164868830000.0\n",
      "Loss =  1850821500.0\n",
      "Grad Norm =  11089000000.0\n",
      "Loss =  635292300.0\n",
      "Grad Norm =  6351063000.0\n",
      "Loss =  975744640.0\n",
      "Grad Norm =  91920100000.0\n",
      "Loss =  460872060.0\n",
      "Grad Norm =  95209280000.0\n",
      "Loss =  458024220.0\n",
      "Grad Norm =  53953670000.0\n",
      "Loss =  499313540.0\n",
      "Grad Norm =  30305524000.0\n",
      "Loss =  352803520.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  24153494000.0\n",
      "Loss =  203955580.0\n",
      "Grad Norm =  57717980000.0\n",
      "Loss =  341862460.0\n",
      "Grad Norm =  53442970000.0\n",
      "Loss =  219562780.0\n",
      "Grad Norm =  48513610000.0\n",
      "Loss =  178158370.0\n",
      "Grad Norm =  40235487000.0\n",
      "Loss =  141144350.0\n",
      "Grad Norm =  23987171000.0\n",
      "Loss =  70145070.0\n",
      "Grad Norm =  8239961000.0\n",
      "Loss =  28755914.0\n",
      "Grad Norm =  10298823000.0\n",
      "Loss =  21179952.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  32547316000.0\n",
      "Loss =  73315090.0\n",
      "Grad Norm =  33908880000.0\n",
      "Loss =  78052800.0\n",
      "Grad Norm =  29349528000.0\n",
      "Loss =  63023760.0\n",
      "Grad Norm =  23746206000.0\n",
      "Loss =  38915040.0\n",
      "Grad Norm =  16907310000.0\n",
      "Loss =  21069756.0\n",
      "Grad Norm =  7200741400.0\n",
      "Loss =  7175521.5\n",
      "Grad Norm =  6075017000.0\n",
      "Loss =  10727616.0\n",
      "Grad Norm =  10290168000.0\n",
      "Loss =  11494108.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  16715652000.0\n",
      "Loss =  21186432.0\n",
      "Grad Norm =  14965416000.0\n",
      "Loss =  16626376.0\n",
      "Grad Norm =  11695628000.0\n",
      "Loss =  11439294.0\n",
      "Grad Norm =  7919782400.0\n",
      "Loss =  8010174.5\n",
      "Grad Norm =  3725425700.0\n",
      "Loss =  4574093.0\n",
      "Grad Norm =  980448450.0\n",
      "Loss =  2344792.5\n",
      "Grad Norm =  5191980000.0\n",
      "Loss =  2903401.5\n",
      "Grad Norm =  4018415900.0\n",
      "Loss =  6130972.0\n",
      "Grad Norm =  11196001000.0\n",
      "Loss =  8562902.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  9510135000.0\n",
      "Loss =  10945675.0\n",
      "Grad Norm =  9588010000.0\n",
      "Loss =  10568338.0\n",
      "Grad Norm =  6410354000.0\n",
      "Loss =  8834956.0\n",
      "Grad Norm =  7455662600.0\n",
      "Loss =  7942283.5\n",
      "Grad Norm =  2150842400.0\n",
      "Loss =  11534074.0\n",
      "Grad Norm =  2851244500.0\n",
      "Loss =  9404501.0\n",
      "Grad Norm =  775087040.0\n",
      "Loss =  12936277.0\n",
      "Grad Norm =  85277336.0\n",
      "Loss =  7802380.5\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  4454297600.0\n",
      "Loss =  14352892.0\n",
      "Grad Norm =  6437025300.0\n",
      "Loss =  20253312.0\n",
      "Grad Norm =  1243776000.0\n",
      "Loss =  10432508.0\n",
      "Grad Norm =  2251336400.0\n",
      "Loss =  21515244.0\n",
      "Grad Norm =  3446679000.0\n",
      "Loss =  13143946.0\n",
      "Grad Norm =  1818561800.0\n",
      "Loss =  25147784.0\n",
      "Grad Norm =  7151224000.0\n",
      "Loss =  18117934.0\n",
      "Grad Norm =  3932559000.0\n",
      "Loss =  30364246.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  1995599600.0\n",
      "Loss =  40526200.0\n",
      "Grad Norm =  10464918000.0\n",
      "Loss =  17541902.0\n",
      "Grad Norm =  7881728000.0\n",
      "Loss =  24879838.0\n",
      "Grad Norm =  3957962500.0\n",
      "Loss =  33342436.0\n",
      "Grad Norm =  4606639600.0\n",
      "Loss =  25566614.0\n",
      "Grad Norm =  67227600.0\n",
      "Loss =  37912250.0\n",
      "Grad Norm =  5046903300.0\n",
      "Loss =  53541292.0\n",
      "Grad Norm =  8909809000.0\n",
      "Loss =  71422190.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  5631786500.0\n",
      "Loss =  75485040.0\n",
      "Grad Norm =  3696841000.0\n",
      "Loss =  50252944.0\n",
      "Grad Norm =  6269536000.0\n",
      "Loss =  53902520.0\n",
      "Grad Norm =  10844524000.0\n",
      "Loss =  44517344.0\n",
      "Grad Norm =  13746551000.0\n",
      "Loss =  32031760.0\n",
      "Grad Norm =  2018680000.0\n",
      "Loss =  76300760.0\n",
      "Grad Norm =  3034232000.0\n",
      "Loss =  98300340.0\n",
      "Grad Norm =  815384800.0\n",
      "Loss =  85386210.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  7266732000.0\n",
      "Loss =  53622616.0\n",
      "Grad Norm =  14746215000.0\n",
      "Loss =  12674083.0\n",
      "Grad Norm =  4151661000.0\n",
      "Loss =  102371100.0\n",
      "Grad Norm =  2166273300.0\n",
      "Loss =  81804920.0\n",
      "Grad Norm =  7696734000.0\n",
      "Loss =  111647550.0\n",
      "Grad Norm =  16010936000.0\n",
      "Loss =  168989420.0\n",
      "Grad Norm =  256488780.0\n",
      "Loss =  87532650.0\n",
      "Epoch Reward = 8.0\n",
      "Grad Norm =  6191323000.0\n",
      "Loss =  91909770.0\n",
      "Grad Norm =  8373320000.0\n",
      "Loss =  94967740.0\n",
      "Grad Norm =  4785310700.0\n",
      "Loss =  120501310.0\n",
      "Grad Norm =  5007771600.0\n",
      "Loss =  123892984.0\n",
      "Grad Norm =  8692099000.0\n",
      "Loss =  101983720.0\n",
      "Grad Norm =  10885287000.0\n",
      "Loss =  75719340.0\n",
      "Grad Norm =  1859921800.0\n",
      "Loss =  102510600.0\n",
      "Grad Norm =  6595113500.0\n",
      "Loss =  39648130.0\n",
      "Grad Norm =  3248144600.0\n",
      "Loss =  73774870.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  3985195000.0\n",
      "Loss =  40110010.0\n",
      "Grad Norm =  13802625000.0\n",
      "Loss =  123256930.0\n",
      "Grad Norm =  10762007000.0\n",
      "Loss =  121547380.0\n",
      "Grad Norm =  19562009000.0\n",
      "Loss =  233829900.0\n",
      "Grad Norm =  6486735400.0\n",
      "Loss =  187698770.0\n",
      "Grad Norm =  16085962000.0\n",
      "Loss =  100814500.0\n",
      "Grad Norm =  20662776000.0\n",
      "Loss =  113735870.0\n",
      "Grad Norm =  2714209500.0\n",
      "Loss =  227646050.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  22380513000.0\n",
      "Loss =  80100730.0\n",
      "Grad Norm =  9019548000.0\n",
      "Loss =  98386530.0\n",
      "Grad Norm =  5918084000.0\n",
      "Loss =  139918000.0\n",
      "Grad Norm =  18978726000.0\n",
      "Loss =  202119920.0\n",
      "Grad Norm =  11320402000.0\n",
      "Loss =  106061470.0\n",
      "Grad Norm =  21660043000.0\n",
      "Loss =  214952030.0\n",
      "Grad Norm =  21475027000.0\n",
      "Loss =  258242670.0\n",
      "Grad Norm =  9015697000.0\n",
      "Loss =  58910388.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  14178645000.0\n",
      "Loss =  170245070.0\n",
      "Grad Norm =  4017928000.0\n",
      "Loss =  289022720.0\n",
      "Grad Norm =  18888145000.0\n",
      "Loss =  236008860.0\n",
      "Grad Norm =  13760515000.0\n",
      "Loss =  273794300.0\n",
      "Grad Norm =  22137932000.0\n",
      "Loss =  204771970.0\n",
      "Grad Norm =  15659178000.0\n",
      "Loss =  191708260.0\n",
      "Grad Norm =  12331524000.0\n",
      "Loss =  133635660.0\n",
      "Grad Norm =  10543644000.0\n",
      "Loss =  243714620.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  4675040000.0\n",
      "Loss =  1258601.0\n",
      "Grad Norm =  16370479000.0\n",
      "Loss =  211576900.0\n",
      "Grad Norm =  20836823000.0\n",
      "Loss =  282137540.0\n",
      "Grad Norm =  7703429600.0\n",
      "Loss =  206934220.0\n",
      "Grad Norm =  359441600.0\n",
      "Loss =  207541500.0\n",
      "Grad Norm =  7494861300.0\n",
      "Loss =  215165700.0\n",
      "Grad Norm =  24869616000.0\n",
      "Loss =  115676370.0\n",
      "Grad Norm =  4943370000.0\n",
      "Loss =  284755970.0\n",
      "Grad Norm =  16196050000.0\n",
      "Loss =  171070670.0\n",
      "Grad Norm =  1601287300.0\n",
      "Loss =  296252000.0\n",
      "Epoch Reward = 11.0\n",
      "Grad Norm =  16313038000.0\n",
      "Loss =  388626900.0\n",
      "Grad Norm =  15695075000.0\n",
      "Loss =  395781660.0\n",
      "Grad Norm =  16991180000.0\n",
      "Loss =  106272560.0\n",
      "Grad Norm =  10020470000.0\n",
      "Loss =  182120660.0\n",
      "Grad Norm =  668598800.0\n",
      "Loss =  260280540.0\n",
      "Grad Norm =  16341953000.0\n",
      "Loss =  425724600.0\n",
      "Grad Norm =  1632521100.0\n",
      "Loss =  271462300.0\n",
      "Epoch Reward = 8.0\n",
      "Grad Norm =  9241875000.0\n",
      "Loss =  447199400.0\n",
      "Grad Norm =  2242556400.0\n",
      "Loss =  376318050.0\n",
      "Grad Norm =  5471189500.0\n",
      "Loss =  386983230.0\n",
      "Grad Norm =  15114584000.0\n",
      "Loss =  319638400.0\n",
      "Grad Norm =  2517804300.0\n",
      "Loss =  486917700.0\n",
      "Grad Norm =  12248936000.0\n",
      "Loss =  327927500.0\n",
      "Grad Norm =  8367871000.0\n",
      "Loss =  329240200.0\n",
      "Grad Norm =  5750929000.0\n",
      "Loss =  428249020.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  5416758300.0\n",
      "Loss =  237874020.0\n",
      "Grad Norm =  10743114000.0\n",
      "Loss =  129585970.0\n",
      "Grad Norm =  32888242000.0\n",
      "Loss =  600318600.0\n",
      "Grad Norm =  10904744000.0\n",
      "Loss =  363575500.0\n",
      "Grad Norm =  13567375000.0\n",
      "Loss =  138389860.0\n",
      "Grad Norm =  24702335000.0\n",
      "Loss =  31014572.0\n",
      "Grad Norm =  10728373000.0\n",
      "Loss =  137927250.0\n",
      "Grad Norm =  15197870000.0\n",
      "Loss =  387146500.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  2115076500.0\n",
      "Loss =  265296130.0\n",
      "Grad Norm =  33336275000.0\n",
      "Loss =  754444100.0\n",
      "Grad Norm =  36019864000.0\n",
      "Loss =  72007880.0\n",
      "Grad Norm =  21119410000.0\n",
      "Loss =  299426370.0\n",
      "Grad Norm =  18958072000.0\n",
      "Loss =  297963620.0\n",
      "Grad Norm =  25718604000.0\n",
      "Loss =  767744300.0\n",
      "Grad Norm =  13533378000.0\n",
      "Loss =  296177020.0\n",
      "Epoch Reward = 8.0\n",
      "Grad Norm =  28806031000.0\n",
      "Loss =  197240160.0\n",
      "Grad Norm =  564048500.0\n",
      "Loss =  560453700.0\n",
      "Grad Norm =  20459060000.0\n",
      "Loss =  813140300.0\n",
      "Grad Norm =  4699464000.0\n",
      "Loss =  576531200.0\n",
      "Grad Norm =  18288507000.0\n",
      "Loss =  470583360.0\n",
      "Grad Norm =  7080356000.0\n",
      "Loss =  592597000.0\n",
      "Grad Norm =  24336940000.0\n",
      "Loss =  351303460.0\n",
      "Grad Norm =  4941660700.0\n",
      "Loss =  471811600.0\n",
      "Grad Norm =  15951960000.0\n",
      "Loss =  626013800.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  15157844000.0\n",
      "Loss =  773004400.0\n",
      "Grad Norm =  17891914000.0\n",
      "Loss =  1018914400.0\n",
      "Grad Norm =  42945778000.0\n",
      "Loss =  610064100.0\n",
      "Grad Norm =  59285262000.0\n",
      "Loss =  555913200.0\n",
      "Grad Norm =  40505434000.0\n",
      "Loss =  611456400.0\n",
      "Grad Norm =  11144880000.0\n",
      "Loss =  685479800.0\n",
      "Grad Norm =  11637846000.0\n",
      "Loss =  375389920.0\n",
      "Epoch Reward = 8.0\n",
      "Grad Norm =  79818520000.0\n",
      "Loss =  1105761400.0\n",
      "Grad Norm =  64152430000.0\n",
      "Loss =  882227200.0\n",
      "Grad Norm =  13781351000.0\n",
      "Loss =  397955520.0\n",
      "Grad Norm =  11422570000.0\n",
      "Loss =  400398400.0\n",
      "Grad Norm =  5000176600.0\n",
      "Loss =  747523100.0\n",
      "Grad Norm =  41439044000.0\n",
      "Loss =  493435140.0\n",
      "Grad Norm =  8207094000.0\n",
      "Loss =  926955300.0\n",
      "Grad Norm =  30408530000.0\n",
      "Loss =  650656450.0\n",
      "Grad Norm =  1904100000.0\n",
      "Loss =  948344000.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  13107332000.0\n",
      "Loss =  814893950.0\n",
      "Grad Norm =  23650275000.0\n",
      "Loss =  841300400.0\n",
      "Grad Norm =  1580202500.0\n",
      "Loss =  443910200.0\n",
      "Grad Norm =  26858312000.0\n",
      "Loss =  868513150.0\n",
      "Grad Norm =  18947664000.0\n",
      "Loss =  864500800.0\n",
      "Grad Norm =  18967087000.0\n",
      "Loss =  1052213000.0\n",
      "Grad Norm =  1482090200.0\n",
      "Loss =  1049195970.0\n",
      "Grad Norm =  11721805000.0\n",
      "Loss =  1382773800.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  70110920000.0\n",
      "Loss =  737949200.0\n",
      "Grad Norm =  8890802000.0\n",
      "Loss =  1428096500.0\n",
      "Grad Norm =  30946157000.0\n",
      "Loss =  760533760.0\n",
      "Grad Norm =  24183595000.0\n",
      "Loss =  524262700.0\n",
      "Grad Norm =  49619517000.0\n",
      "Loss =  1230142700.0\n",
      "Grad Norm =  15163379000.0\n",
      "Loss =  519307970.0\n",
      "Grad Norm =  34102630000.0\n",
      "Loss =  790996900.0\n",
      "Grad Norm =  38880686000.0\n",
      "Loss =  1032471100.0\n",
      "Grad Norm =  13740915000.0\n",
      "Loss =  536111040.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  50306916000.0\n",
      "Loss =  389586560.0\n",
      "Grad Norm =  15646547000.0\n",
      "Loss =  1239715600.0\n",
      "Grad Norm =  32914782000.0\n",
      "Loss =  1481507600.0\n",
      "Grad Norm =  49916070000.0\n",
      "Loss =  394988400.0\n",
      "Grad Norm =  12738504000.0\n",
      "Loss =  825258940.0\n",
      "Grad Norm =  1309674800.0\n",
      "Loss =  826914800.0\n",
      "Grad Norm =  5783775000.0\n",
      "Loss =  576765060.0\n",
      "Epoch Reward = 8.0\n",
      "Grad Norm =  42515200000.0\n",
      "Loss =  1158903300.0\n",
      "Grad Norm =  28384555000.0\n",
      "Loss =  1134314400.0\n",
      "Grad Norm =  9464990000.0\n",
      "Loss =  868854400.0\n",
      "Grad Norm =  71181070000.0\n",
      "Loss =  211288980.0\n",
      "Grad Norm =  15333726000.0\n",
      "Loss =  890081860.0\n",
      "Grad Norm =  31126747000.0\n",
      "Loss =  1415779800.0\n",
      "Grad Norm =  33228069000.0\n",
      "Loss =  1432659700.0\n",
      "Grad Norm =  38973886000.0\n",
      "Loss =  1680314900.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  1852602800.0\n",
      "Loss =  1648381800.0\n",
      "Grad Norm =  57056006000.0\n",
      "Loss =  1073737860.0\n",
      "Grad Norm =  20512674000.0\n",
      "Loss =  1467969000.0\n",
      "Grad Norm =  7159971000.0\n",
      "Loss =  1688935200.0\n",
      "Grad Norm =  16173765000.0\n",
      "Loss =  1709592600.0\n",
      "Grad Norm =  46819120000.0\n",
      "Loss =  747311200.0\n",
      "Grad Norm =  12718210000.0\n",
      "Loss =  967042900.0\n",
      "Grad Norm =  9409373000.0\n",
      "Loss =  970760600.0\n",
      "Grad Norm =  9234734000.0\n",
      "Loss =  350092830.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  22183686000.0\n",
      "Loss =  1004936500.0\n",
      "Grad Norm =  24411500000.0\n",
      "Loss =  708194300.0\n",
      "Grad Norm =  56815698000.0\n",
      "Loss =  483580300.0\n",
      "Grad Norm =  75048536.0\n",
      "Loss =  1296905700.0\n",
      "Grad Norm =  12482566000.0\n",
      "Loss =  1019799700.0\n",
      "Grad Norm =  14176477000.0\n",
      "Loss =  1320991500.0\n",
      "Grad Norm =  71150270000.0\n",
      "Loss =  2249585200.0\n",
      "Grad Norm =  23883434000.0\n",
      "Loss =  1055428800.0\n",
      "Grad Norm =  52561826000.0\n",
      "Loss =  828224100.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  28552606000.0\n",
      "Loss =  1918107400.0\n",
      "Grad Norm =  13043003000.0\n",
      "Loss =  1369199400.0\n",
      "Grad Norm =  14245942000.0\n",
      "Loss =  1378468900.0\n",
      "Grad Norm =  28385640000.0\n",
      "Loss =  1101945300.0\n",
      "Grad Norm =  2994472200.0\n",
      "Loss =  1386697600.0\n",
      "Grad Norm =  22663250000.0\n",
      "Loss =  765279550.0\n",
      "Grad Norm =  15752959000.0\n",
      "Loss =  1097614200.0\n",
      "Grad Norm =  104337390000.0\n",
      "Loss =  2571441200.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  39756680000.0\n",
      "Loss =  822558200.0\n",
      "Grad Norm =  22462850000.0\n",
      "Loss =  1133025800.0\n",
      "Grad Norm =  34898756000.0\n",
      "Loss =  818252540.0\n",
      "Grad Norm =  1633003900.0\n",
      "Loss =  1124990300.0\n",
      "Grad Norm =  898553100.0\n",
      "Loss =  779760700.0\n",
      "Grad Norm =  55111553000.0\n",
      "Loss =  1562340600.0\n",
      "Grad Norm =  12651694000.0\n",
      "Loss =  412194530.0\n",
      "Grad Norm =  5071177000.0\n",
      "Loss =  792041800.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  25430661000.0\n",
      "Loss =  821948900.0\n",
      "Grad Norm =  19769830000.0\n",
      "Loss =  1177553400.0\n",
      "Grad Norm =  24664408000.0\n",
      "Loss =  1191083800.0\n",
      "Grad Norm =  1668425200.0\n",
      "Loss =  1509880800.0\n",
      "Grad Norm =  21858210000.0\n",
      "Loss =  1845275300.0\n",
      "Grad Norm =  20368685000.0\n",
      "Loss =  1198224100.0\n",
      "Grad Norm =  16795989000.0\n",
      "Loss =  1197780000.0\n",
      "Grad Norm =  12196976000.0\n",
      "Loss =  1539596400.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  41471400000.0\n",
      "Loss =  1923090700.0\n",
      "Grad Norm =  29238092000.0\n",
      "Loss =  1900604000.0\n",
      "Grad Norm =  68609680000.0\n",
      "Loss =  610879300.0\n",
      "Grad Norm =  14189156000.0\n",
      "Loss =  1890586400.0\n",
      "Grad Norm =  34088186000.0\n",
      "Loss =  2227099100.0\n",
      "Grad Norm =  73802650000.0\n",
      "Loss =  645775550.0\n",
      "Grad Norm =  39894897000.0\n",
      "Loss =  904223360.0\n",
      "Grad Norm =  31082676000.0\n",
      "Loss =  1616733800.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  24021811000.0\n",
      "Loss =  874686800.0\n",
      "Grad Norm =  60285243000.0\n",
      "Loss =  575426000.0\n",
      "Grad Norm =  53517580000.0\n",
      "Loss =  546664400.0\n",
      "Grad Norm =  14979459000.0\n",
      "Loss =  1260310000.0\n",
      "Grad Norm =  14529031000.0\n",
      "Loss =  874050050.0\n",
      "Grad Norm =  4976479700.0\n",
      "Loss =  450582530.0\n",
      "Grad Norm =  124000720000.0\n",
      "Loss =  2732401400.0\n",
      "Grad Norm =  10660956000.0\n",
      "Loss =  880685900.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  17039226000.0\n",
      "Loss =  2305787000.0\n",
      "Grad Norm =  43612770000.0\n",
      "Loss =  1731222300.0\n",
      "Grad Norm =  48569352000.0\n",
      "Loss =  1755785000.0\n",
      "Grad Norm =  18266819000.0\n",
      "Loss =  2017519200.0\n",
      "Grad Norm =  60103530000.0\n",
      "Loss =  1032614900.0\n",
      "Grad Norm =  64814700000.0\n",
      "Loss =  144719600.0\n",
      "Grad Norm =  90583720000.0\n",
      "Loss =  1918437600.0\n",
      "Grad Norm =  162479080000.0\n",
      "Loss =  3080108000.0\n",
      "Grad Norm =  56991638000.0\n",
      "Loss =  1791646600.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  70541480000.0\n",
      "Loss =  1516882300.0\n",
      "Grad Norm =  40549982000.0\n",
      "Loss =  2122116900.0\n",
      "Grad Norm =  35290173000.0\n",
      "Loss =  2113843200.0\n",
      "Grad Norm =  78811230000.0\n",
      "Loss =  1172779000.0\n",
      "Grad Norm =  21194410000.0\n",
      "Loss =  2087281400.0\n",
      "Grad Norm =  8055722500.0\n",
      "Loss =  924895500.0\n",
      "Grad Norm =  117362890000.0\n",
      "Loss =  2828568000.0\n",
      "Grad Norm =  32323727000.0\n",
      "Loss =  1378326700.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  41620783000.0\n",
      "Loss =  2485588700.0\n",
      "Grad Norm =  72909600000.0\n",
      "Loss =  1148726800.0\n",
      "Grad Norm =  39050777000.0\n",
      "Loss =  1808633300.0\n",
      "Grad Norm =  91986660000.0\n",
      "Loss =  817363700.0\n",
      "Grad Norm =  68477747000.0\n",
      "Loss =  2906893800.0\n",
      "Grad Norm =  17678862000.0\n",
      "Loss =  1766180600.0\n",
      "Grad Norm =  21578197000.0\n",
      "Loss =  1774537500.0\n",
      "Grad Norm =  79698300000.0\n",
      "Loss =  2979058200.0\n",
      "Grad Norm =  15661662000.0\n",
      "Loss =  1772842500.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  16697447000.0\n",
      "Loss =  2473414100.0\n",
      "Grad Norm =  83315550000.0\n",
      "Loss =  1691437700.0\n",
      "Grad Norm =  13185763000.0\n",
      "Loss =  2471508000.0\n",
      "Grad Norm =  72870306000.0\n",
      "Loss =  1171424800.0\n",
      "Grad Norm =  34886530000.0\n",
      "Loss =  993165600.0\n",
      "Grad Norm =  42205490000.0\n",
      "Loss =  1433116000.0\n",
      "Grad Norm =  82097860000.0\n",
      "Loss =  1575136900.0\n",
      "Grad Norm =  125498590000.0\n",
      "Loss =  2607490300.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  46556320000.0\n",
      "Loss =  2863049700.0\n",
      "Grad Norm =  102644425000.0\n",
      "Loss =  1456168800.0\n",
      "Grad Norm =  46226910000.0\n",
      "Loss =  2586459600.0\n",
      "Grad Norm =  48523980000.0\n",
      "Loss =  2605356000.0\n",
      "Grad Norm =  110585190000.0\n",
      "Loss =  1583402200.0\n",
      "Grad Norm =  3390212400.0\n",
      "Loss =  2485950500.0\n",
      "Grad Norm =  37135700000.0\n",
      "Loss =  2543464000.0\n",
      "Grad Norm =  23977232000.0\n",
      "Loss =  1810039600.0\n",
      "Grad Norm =  22673811000.0\n",
      "Loss =  513977540.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  49182953000.0\n",
      "Loss =  1475335800.0\n",
      "Grad Norm =  55114980000.0\n",
      "Loss =  1901532500.0\n",
      "Grad Norm =  1801325600.0\n",
      "Loss =  1400532000.0\n",
      "Grad Norm =  31611826000.0\n",
      "Loss =  2554327300.0\n",
      "Grad Norm =  77394190000.0\n",
      "Loss =  1236949500.0\n",
      "Grad Norm =  5703917600.0\n",
      "Loss =  2521760300.0\n",
      "Grad Norm =  64889140000.0\n",
      "Loss =  1595985400.0\n",
      "Epoch Reward = 8.0\n",
      "Grad Norm =  96802840000.0\n",
      "Loss =  375441060.0\n",
      "Grad Norm =  52276320000.0\n",
      "Loss =  2288577000.0\n",
      "Grad Norm =  30505742000.0\n",
      "Loss =  30638742.0\n",
      "Grad Norm =  76151840000.0\n",
      "Loss =  1592872700.0\n",
      "Grad Norm =  102967930000.0\n",
      "Loss =  2146563800.0\n",
      "Grad Norm =  16167047000.0\n",
      "Loss =  989673500.0\n",
      "Grad Norm =  24075608.0\n",
      "Loss =  1424281900.0\n",
      "Grad Norm =  29093750000.0\n",
      "Loss =  1459502600.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  812216260.0\n",
      "Loss =  2214336500.0\n",
      "Grad Norm =  14269768000.0\n",
      "Loss =  1846228700.0\n",
      "Grad Norm =  25645259000.0\n",
      "Loss =  1454705300.0\n",
      "Grad Norm =  30130640000.0\n",
      "Loss =  1020315500.0\n",
      "Grad Norm =  80752790000.0\n",
      "Loss =  2797447000.0\n",
      "Grad Norm =  33543307000.0\n",
      "Loss =  1881580700.0\n",
      "Grad Norm =  23738317000.0\n",
      "Loss =  1007609340.0\n",
      "Grad Norm =  8224261000.0\n",
      "Loss =  1432667900.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  8657199000.0\n",
      "Loss =  1432835200.0\n",
      "Grad Norm =  37045793000.0\n",
      "Loss =  2271217700.0\n",
      "Grad Norm =  7800185000.0\n",
      "Loss =  1842741500.0\n",
      "Grad Norm =  38249193000.0\n",
      "Loss =  2621828600.0\n",
      "Grad Norm =  61051654000.0\n",
      "Loss =  1144859900.0\n",
      "Grad Norm =  60300247000.0\n",
      "Loss =  1140706400.0\n",
      "Grad Norm =  22403035000.0\n",
      "Loss =  1447668000.0\n",
      "Grad Norm =  17897896000.0\n",
      "Loss =  996213400.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  16776690000.0\n",
      "Loss =  517657760.0\n",
      "Grad Norm =  102556705000.0\n",
      "Loss =  2167981000.0\n",
      "Grad Norm =  31237384000.0\n",
      "Loss =  1016768700.0\n",
      "Grad Norm =  41750220000.0\n",
      "Loss =  1899885300.0\n",
      "Grad Norm =  18815744000.0\n",
      "Loss =  1440528800.0\n",
      "Grad Norm =  33285954000.0\n",
      "Loss =  2915735600.0\n",
      "Grad Norm =  53248193000.0\n",
      "Loss =  1971723800.0\n",
      "Grad Norm =  99817325000.0\n",
      "Loss =  1490402800.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  25696553000.0\n",
      "Loss =  1858397400.0\n",
      "Grad Norm =  31951309000.0\n",
      "Loss =  1020288100.0\n",
      "Grad Norm =  120004660000.0\n",
      "Loss =  3366101800.0\n",
      "Grad Norm =  100010830000.0\n",
      "Loss =  2899147800.0\n",
      "Grad Norm =  17827360000.0\n",
      "Loss =  990971400.0\n",
      "Grad Norm =  1787631200.0\n",
      "Loss =  1825998100.0\n",
      "Grad Norm =  35623444000.0\n",
      "Loss =  1472692900.0\n",
      "Grad Norm =  21893243000.0\n",
      "Loss =  1844441600.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  8859045000.0\n",
      "Loss =  1823724000.0\n",
      "Grad Norm =  74192230000.0\n",
      "Loss =  226549340.0\n",
      "Grad Norm =  43899077000.0\n",
      "Loss =  1890251100.0\n",
      "Grad Norm =  66539150000.0\n",
      "Loss =  1973960700.0\n",
      "Grad Norm =  105058980000.0\n",
      "Loss =  2924192500.0\n",
      "Grad Norm =  59013657000.0\n",
      "Loss =  137212380.0\n",
      "Grad Norm =  28176204000.0\n",
      "Loss =  2221402600.0\n",
      "Grad Norm =  30834465000.0\n",
      "Loss =  1450835300.0\n",
      "Grad Norm =  18458431000.0\n",
      "Loss =  2540817700.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  61794520000.0\n",
      "Loss =  1143166100.0\n",
      "Grad Norm =  251636590.0\n",
      "Loss =  1809467600.0\n",
      "Grad Norm =  85226610000.0\n",
      "Loss =  3112278000.0\n",
      "Grad Norm =  20652575000.0\n",
      "Loss =  985943100.0\n",
      "Grad Norm =  18460178000.0\n",
      "Loss =  982242050.0\n",
      "Grad Norm =  9610392000.0\n",
      "Loss =  1408106600.0\n",
      "Grad Norm =  54741766000.0\n",
      "Loss =  2294493700.0\n",
      "Grad Norm =  40167870000.0\n",
      "Loss =  2243566000.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  78690800000.0\n",
      "Loss =  782186240.0\n",
      "Grad Norm =  39628510000.0\n",
      "Loss =  2892031500.0\n",
      "Grad Norm =  79558880000.0\n",
      "Loss =  789562240.0\n",
      "Grad Norm =  7205485000.0\n",
      "Loss =  1806719200.0\n",
      "Grad Norm =  48772090000.0\n",
      "Loss =  597395840.0\n",
      "Grad Norm =  23504650000.0\n",
      "Loss =  1422247700.0\n",
      "Grad Norm =  69475610000.0\n",
      "Loss =  1974113300.0\n",
      "Grad Norm =  45535720000.0\n",
      "Loss =  1475414100.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  30768222000.0\n",
      "Loss =  536533470.0\n",
      "Grad Norm =  4739800000.0\n",
      "Loss =  1403176400.0\n",
      "Grad Norm =  1295832000.0\n",
      "Loss =  1402311000.0\n",
      "Grad Norm =  6206915600.0\n",
      "Loss =  1403745500.0\n",
      "Grad Norm =  11078585000.0\n",
      "Loss =  1810023200.0\n",
      "Grad Norm =  14136080000.0\n",
      "Loss =  1410277900.0\n",
      "Grad Norm =  24322132000.0\n",
      "Loss =  2199963400.0\n",
      "Grad Norm =  3435783700.0\n",
      "Loss =  1804894700.0\n",
      "Grad Norm =  47869678000.0\n",
      "Loss =  1066648450.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  51423674000.0\n",
      "Loss =  609315200.0\n",
      "Grad Norm =  72046220000.0\n",
      "Loss =  2715262000.0\n",
      "Grad Norm =  9663811000.0\n",
      "Loss =  1406050400.0\n",
      "Grad Norm =  5371235300.0\n",
      "Loss =  1404335000.0\n",
      "Grad Norm =  40026665000.0\n",
      "Loss =  2242707000.0\n",
      "Grad Norm =  16667766000.0\n",
      "Loss =  1416051500.0\n",
      "Grad Norm =  64646185000.0\n",
      "Loss =  681615300.0\n",
      "Grad Norm =  16321388000.0\n",
      "Loss =  1416302100.0\n",
      "Grad Norm =  17749357000.0\n",
      "Loss =  1822396300.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  51592946000.0\n",
      "Loss =  2287207400.0\n",
      "Grad Norm =  39395336000.0\n",
      "Loss =  2247055400.0\n",
      "Grad Norm =  16847967000.0\n",
      "Loss =  2197465600.0\n",
      "Grad Norm =  25039954000.0\n",
      "Loss =  1841843500.0\n",
      "Grad Norm =  37158232000.0\n",
      "Loss =  1879070600.0\n",
      "Grad Norm =  1991648800.0\n",
      "Loss =  2524718300.0\n",
      "Grad Norm =  39387330000.0\n",
      "Loss =  1887391000.0\n",
      "Grad Norm =  30379602000.0\n",
      "Loss =  1855408400.0\n",
      "Grad Norm =  24496544000.0\n",
      "Loss =  2550928000.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  17197855000.0\n",
      "Loss =  1825458800.0\n",
      "Grad Norm =  73624780000.0\n",
      "Loss =  2737651200.0\n",
      "Grad Norm =  17762808000.0\n",
      "Loss =  985109760.0\n",
      "Grad Norm =  39429714000.0\n",
      "Loss =  2251689000.0\n",
      "Grad Norm =  15310890000.0\n",
      "Loss =  1421037200.0\n",
      "Grad Norm =  23369796000.0\n",
      "Loss =  1434997800.0\n",
      "Grad Norm =  23898294000.0\n",
      "Loss =  1436378100.0\n",
      "Grad Norm =  37156910000.0\n",
      "Loss =  1032261060.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  58558857000.0\n",
      "Loss =  1944104300.0\n",
      "Grad Norm =  31287542000.0\n",
      "Loss =  1010266100.0\n",
      "Grad Norm =  33552340000.0\n",
      "Loss =  1016359040.0\n",
      "Grad Norm =  108866540000.0\n",
      "Loss =  2965095200.0\n",
      "Grad Norm =  924060000.0\n",
      "Loss =  1418509000.0\n",
      "Grad Norm =  35315577000.0\n",
      "Loss =  1474561700.0\n",
      "Grad Norm =  16429443000.0\n",
      "Loss =  2214278700.0\n",
      "Grad Norm =  10251578000.0\n",
      "Loss =  2548579800.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  21182038000.0\n",
      "Loss =  2222196200.0\n",
      "Grad Norm =  20160262000.0\n",
      "Loss =  1844188500.0\n",
      "Grad Norm =  34376950000.0\n",
      "Loss =  1028020200.0\n",
      "Grad Norm =  21045316000.0\n",
      "Loss =  1437264800.0\n",
      "Grad Norm =  49565225000.0\n",
      "Loss =  1509900500.0\n",
      "Grad Norm =  79360926000.0\n",
      "Loss =  2054507300.0\n",
      "Grad Norm =  56775400000.0\n",
      "Loss =  1953793700.0\n",
      "Grad Norm =  699438660.0\n",
      "Loss =  1425132800.0\n",
      "Grad Norm =  48191726000.0\n",
      "Loss =  1086947300.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  19579180000.0\n",
      "Loss =  1852442800.0\n",
      "Grad Norm =  5563778600.0\n",
      "Loss =  2212827600.0\n",
      "Grad Norm =  13871323000.0\n",
      "Loss =  1838251000.0\n",
      "Grad Norm =  98687710000.0\n",
      "Loss =  606309400.0\n",
      "Grad Norm =  317394060000.0\n",
      "Loss =  3089061600.0\n",
      "Grad Norm =  4994583000.0\n",
      "Loss =  2213282300.0\n",
      "Grad Norm =  83041450000.0\n",
      "Loss =  1314194600.0\n",
      "Grad Norm =  31391078000.0\n",
      "Loss =  2265695500.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  85031780000.0\n",
      "Loss =  834647700.0\n",
      "Grad Norm =  1265676500.0\n",
      "Loss =  1437911200.0\n",
      "Grad Norm =  69368980000.0\n",
      "Loss =  2012209200.0\n",
      "Grad Norm =  63752147000.0\n",
      "Loss =  1573472300.0\n",
      "Grad Norm =  51693724000.0\n",
      "Loss =  1537144800.0\n",
      "Grad Norm =  87000920000.0\n",
      "Loss =  2872707600.0\n",
      "Grad Norm =  85072890000.0\n",
      "Loss =  765050300.0\n",
      "Grad Norm =  14370170000.0\n",
      "Loss =  2292147500.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  53343736000.0\n",
      "Loss =  1586873900.0\n",
      "Grad Norm =  19337996000.0\n",
      "Loss =  1500234200.0\n",
      "Grad Norm =  1687032400.0\n",
      "Loss =  1029686900.0\n",
      "Grad Norm =  63427895000.0\n",
      "Loss =  1618200600.0\n",
      "Grad Norm =  95367910000.0\n",
      "Loss =  2203462700.0\n",
      "Grad Norm =  54762990000.0\n",
      "Loss =  2037738900.0\n",
      "Grad Norm =  1661043200.0\n",
      "Loss =  1951295000.0\n",
      "Grad Norm =  22319653000.0\n",
      "Loss =  2378041300.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  115908030000.0\n",
      "Loss =  1631980300.0\n",
      "Grad Norm =  88195030000.0\n",
      "Loss =  1363213700.0\n",
      "Grad Norm =  73120104000.0\n",
      "Loss =  180432720.0\n",
      "Grad Norm =  178823350000.0\n",
      "Loss =  3640449000.0\n",
      "Grad Norm =  142736020000.0\n",
      "Loss =  2537086500.0\n",
      "Grad Norm =  56723436000.0\n",
      "Loss =  1647870700.0\n",
      "Grad Norm =  33596082000.0\n",
      "Loss =  2453386500.0\n",
      "Grad Norm =  50153787000.0\n",
      "Loss =  2110505200.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  60895023000.0\n",
      "Loss =  2588444700.0\n",
      "Grad Norm =  42188833000.0\n",
      "Loss =  2505511700.0\n",
      "Grad Norm =  32336652000.0\n",
      "Loss =  2061625600.0\n",
      "Grad Norm =  35518525000.0\n",
      "Loss =  1127852000.0\n",
      "Grad Norm =  23566778000.0\n",
      "Loss =  1104751100.0\n",
      "Grad Norm =  72853275000.0\n",
      "Loss =  1231715700.0\n",
      "Grad Norm =  159855170000.0\n",
      "Loss =  3139752000.0\n",
      "Grad Norm =  64550257000.0\n",
      "Loss =  2173748200.0\n",
      "Grad Norm =  11065002000.0\n",
      "Loss =  2056845300.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  64735097000.0\n",
      "Loss =  2236688000.0\n",
      "Grad Norm =  8887309000.0\n",
      "Loss =  2878248200.0\n",
      "Grad Norm =  14472599000.0\n",
      "Loss =  2503102200.0\n",
      "Grad Norm =  62075613000.0\n",
      "Loss =  1254627100.0\n",
      "Grad Norm =  217036560.0\n",
      "Loss =  1619314000.0\n",
      "Grad Norm =  39948710000.0\n",
      "Loss =  1674393000.0\n",
      "Grad Norm =  61699355000.0\n",
      "Loss =  1743801300.0\n",
      "Grad Norm =  133407740000.0\n",
      "Loss =  3444795400.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  23239866000.0\n",
      "Loss =  2590287400.0\n",
      "Grad Norm =  21147034000.0\n",
      "Loss =  2999751700.0\n",
      "Grad Norm =  52009530000.0\n",
      "Loss =  2252963300.0\n",
      "Grad Norm =  59312670000.0\n",
      "Loss =  2295770600.0\n",
      "Grad Norm =  117187445000.0\n",
      "Loss =  1117963500.0\n",
      "Grad Norm =  24546953000.0\n",
      "Loss =  1716932600.0\n",
      "Grad Norm =  15584676000.0\n",
      "Loss =  616367700.0\n",
      "Epoch Reward = 8.0\n",
      "Grad Norm =  49822036000.0\n",
      "Loss =  1266475900.0\n",
      "Grad Norm =  80179100000.0\n",
      "Loss =  2905326600.0\n",
      "Grad Norm =  42416837000.0\n",
      "Loss =  1823765500.0\n",
      "Grad Norm =  127175100000.0\n",
      "Loss =  1217537000.0\n",
      "Grad Norm =  23168883000.0\n",
      "Loss =  2787110000.0\n",
      "Grad Norm =  81587175000.0\n",
      "Loss =  1457861100.0\n",
      "Grad Norm =  63855220000.0\n",
      "Loss =  763733400.0\n",
      "Grad Norm =  66234384000.0\n",
      "Loss =  1926345500.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  51127540000.0\n",
      "Loss =  1329244800.0\n",
      "Grad Norm =  26810284000.0\n",
      "Loss =  675195900.0\n",
      "Grad Norm =  25800395000.0\n",
      "Loss =  2395592200.0\n",
      "Grad Norm =  16506702000.0\n",
      "Loss =  2880993800.0\n",
      "Grad Norm =  94187184000.0\n",
      "Loss =  1579210000.0\n",
      "Grad Norm =  15390673000.0\n",
      "Loss =  2893537300.0\n",
      "Grad Norm =  32303806000.0\n",
      "Loss =  2433645000.0\n",
      "Grad Norm =  38753714000.0\n",
      "Loss =  1915402000.0\n",
      "Grad Norm =  59345162000.0\n",
      "Loss =  769290240.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  133289050000.0\n",
      "Loss =  3375586800.0\n",
      "Grad Norm =  81523440000.0\n",
      "Loss =  3120281000.0\n",
      "Grad Norm =  19132103000.0\n",
      "Loss =  2454221300.0\n",
      "Grad Norm =  64751104000.0\n",
      "Loss =  2599561000.0\n",
      "Grad Norm =  80861700000.0\n",
      "Loss =  2699670000.0\n",
      "Grad Norm =  95804380000.0\n",
      "Loss =  2245438700.0\n",
      "Grad Norm =  133214800000.0\n",
      "Loss =  592799740.0\n",
      "Epoch Reward = 8.0\n",
      "Grad Norm =  119198560000.0\n",
      "Loss =  1645187100.0\n",
      "Grad Norm =  157603300000.0\n",
      "Loss =  1866979800.0\n",
      "Grad Norm =  161502070000.0\n",
      "Loss =  3106277400.0\n",
      "Grad Norm =  120628175000.0\n",
      "Loss =  4327461000.0\n",
      "Grad Norm =  79815630000.0\n",
      "Loss =  2753469700.0\n",
      "Grad Norm =  109744570000.0\n",
      "Loss =  3595795000.0\n",
      "Grad Norm =  188530480000.0\n",
      "Loss =  3190077200.0\n",
      "Grad Norm =  149142720000.0\n",
      "Loss =  2995704300.0\n",
      "Grad Norm =  117872580000.0\n",
      "Loss =  1873891800.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  153201970000.0\n",
      "Loss =  1842523600.0\n",
      "Grad Norm =  283222640000.0\n",
      "Loss =  3463416000.0\n",
      "Grad Norm =  218232500000.0\n",
      "Loss =  3541285600.0\n",
      "Grad Norm =  41468830000.0\n",
      "Loss =  2585010400.0\n",
      "Grad Norm =  76936315000.0\n",
      "Loss =  2757439700.0\n",
      "Grad Norm =  157184820000.0\n",
      "Loss =  3016272000.0\n",
      "Grad Norm =  117675610000.0\n",
      "Loss =  3673650700.0\n",
      "Grad Norm =  168420900000.0\n",
      "Loss =  2542852000.0\n",
      "Grad Norm =  134644410000.0\n",
      "Loss =  1328117400.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  149726280000.0\n",
      "Loss =  2486277400.0\n",
      "Grad Norm =  324623140000.0\n",
      "Loss =  6712194600.0\n",
      "Grad Norm =  44224238000.0\n",
      "Loss =  769920830.0\n",
      "Grad Norm =  39858004000.0\n",
      "Loss =  2645645300.0\n",
      "Grad Norm =  138394170000.0\n",
      "Loss =  2073370400.0\n",
      "Grad Norm =  187366060000.0\n",
      "Loss =  1251869400.0\n",
      "Grad Norm =  85945030000.0\n",
      "Loss =  4682841000.0\n",
      "Grad Norm =  8159674000.0\n",
      "Loss =  2600935000.0\n",
      "Grad Norm =  51349004000.0\n",
      "Loss =  792279230.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  32450304000.0\n",
      "Loss =  1418813700.0\n",
      "Grad Norm =  23140330000.0\n",
      "Loss =  1404369200.0\n",
      "Grad Norm =  54364172000.0\n",
      "Loss =  77614760.0\n",
      "Grad Norm =  13533418000.0\n",
      "Loss =  1389121200.0\n",
      "Grad Norm =  75491670000.0\n",
      "Loss =  2723433700.0\n",
      "Grad Norm =  15129945000.0\n",
      "Loss =  1382570000.0\n",
      "Grad Norm =  62622390000.0\n",
      "Loss =  820315650.0\n",
      "Grad Norm =  2483178500.0\n",
      "Loss =  1981439500.0\n",
      "Grad Norm =  38363152000.0\n",
      "Loss =  2581177000.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  89213020000.0\n",
      "Loss =  3730354200.0\n",
      "Grad Norm =  82791980000.0\n",
      "Loss =  899658400.0\n",
      "Grad Norm =  23850109000.0\n",
      "Loss =  1955886800.0\n",
      "Grad Norm =  10563310000.0\n",
      "Loss =  1932961500.0\n",
      "Grad Norm =  77791320000.0\n",
      "Loss =  169805630.0\n",
      "Grad Norm =  57209434000.0\n",
      "Loss =  1997034800.0\n",
      "Grad Norm =  138300520000.0\n",
      "Loss =  3434522000.0\n",
      "Grad Norm =  118830110000.0\n",
      "Loss =  3785888000.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  101912490000.0\n",
      "Loss =  1674012500.0\n",
      "Grad Norm =  100260630000.0\n",
      "Loss =  1659226900.0\n",
      "Grad Norm =  81457300000.0\n",
      "Loss =  4336542700.0\n",
      "Grad Norm =  65870537000.0\n",
      "Loss =  1425522700.0\n",
      "Grad Norm =  11616938000.0\n",
      "Loss =  2385313300.0\n",
      "Grad Norm =  60026425000.0\n",
      "Loss =  2968622600.0\n",
      "Grad Norm =  3966034200.0\n",
      "Loss =  1837268700.0\n",
      "Grad Norm =  25961853000.0\n",
      "Loss =  1282970400.0\n",
      "Grad Norm =  19704996000.0\n",
      "Loss =  1269418800.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  77646990000.0\n",
      "Loss =  2988596700.0\n",
      "Grad Norm =  478033660.0\n",
      "Loss =  1804208900.0\n",
      "Grad Norm =  4066814700.0\n",
      "Loss =  2314182400.0\n",
      "Grad Norm =  63114523000.0\n",
      "Loss =  1373999700.0\n",
      "Grad Norm =  10271465000.0\n",
      "Loss =  2298434000.0\n",
      "Grad Norm =  51652047000.0\n",
      "Loss =  1315074300.0\n",
      "Grad Norm =  22758257000.0\n",
      "Loss =  2293564200.0\n",
      "Grad Norm =  37953740000.0\n",
      "Loss =  672812160.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  67665110000.0\n",
      "Loss =  2392165000.0\n",
      "Grad Norm =  90959060000.0\n",
      "Loss =  3393823700.0\n",
      "Grad Norm =  35530030000.0\n",
      "Loss =  3162302700.0\n",
      "Grad Norm =  58237510000.0\n",
      "Loss =  2369884700.0\n",
      "Grad Norm =  100084920000.0\n",
      "Loss =  2183971800.0\n",
      "Grad Norm =  95497880000.0\n",
      "Loss =  2128882700.0\n",
      "Grad Norm =  25621109000.0\n",
      "Loss =  2668270000.0\n",
      "Grad Norm =  56691675000.0\n",
      "Loss =  1290516700.0\n",
      "Grad Norm =  2388596700.0\n",
      "Loss =  1164227100.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  83218230000.0\n",
      "Loss =  1345303000.0\n",
      "Grad Norm =  94429970000.0\n",
      "Loss =  1925985700.0\n",
      "Grad Norm =  19356723000.0\n",
      "Loss =  1162074900.0\n",
      "Grad Norm =  24394926000.0\n",
      "Loss =  1166736900.0\n",
      "Grad Norm =  28439368000.0\n",
      "Loss =  1685232400.0\n",
      "Grad Norm =  42583610000.0\n",
      "Loss =  1719587300.0\n",
      "Grad Norm =  21657186000.0\n",
      "Loss =  2129766700.0\n",
      "Grad Norm =  5456470000.0\n",
      "Loss =  2534526200.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  7175040500.0\n",
      "Loss =  2086214100.0\n",
      "Grad Norm =  92812400000.0\n",
      "Loss =  3549877500.0\n",
      "Grad Norm =  10332104000.0\n",
      "Loss =  2072081400.0\n",
      "Grad Norm =  69755090000.0\n",
      "Loss =  757913000.0\n",
      "Grad Norm =  25392870000.0\n",
      "Loss =  2496977000.0\n",
      "Grad Norm =  40818946000.0\n",
      "Loss =  1157952500.0\n",
      "Grad Norm =  7186785300.0\n",
      "Loss =  1582875300.0\n",
      "Grad Norm =  28477608000.0\n",
      "Loss =  2057167600.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  10503469000.0\n",
      "Loss =  2017105900.0\n",
      "Grad Norm =  32191384000.0\n",
      "Loss =  2834585300.0\n",
      "Grad Norm =  34202282000.0\n",
      "Loss =  2047820800.0\n",
      "Grad Norm =  104464160000.0\n",
      "Loss =  1056442000.0\n",
      "Grad Norm =  71320660000.0\n",
      "Loss =  1285734400.0\n",
      "Grad Norm =  64740530000.0\n",
      "Loss =  3247783200.0\n",
      "Grad Norm =  5259673000.0\n",
      "Loss =  1526702300.0\n",
      "Grad Norm =  96229564000.0\n",
      "Loss =  3402790400.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  3389809200.0\n",
      "Loss =  1943756800.0\n",
      "Grad Norm =  68195830000.0\n",
      "Loss =  731315600.0\n",
      "Grad Norm =  18620678000.0\n",
      "Loss =  1512527200.0\n",
      "Grad Norm =  25721102000.0\n",
      "Loss =  1055873150.0\n",
      "Grad Norm =  16144292000.0\n",
      "Loss =  1498159600.0\n",
      "Grad Norm =  31207030000.0\n",
      "Loss =  1519035800.0\n",
      "Grad Norm =  13055840000.0\n",
      "Loss =  1026562400.0\n",
      "Grad Norm =  33903840000.0\n",
      "Loss =  1516669800.0\n",
      "Grad Norm =  821983200.0\n",
      "Loss =  1014447600.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  46887272000.0\n",
      "Loss =  609780600.0\n",
      "Grad Norm =  11781577000.0\n",
      "Loss =  1880707300.0\n",
      "Grad Norm =  6427672600.0\n",
      "Loss =  1870349600.0\n",
      "Grad Norm =  39566246000.0\n",
      "Loss =  2660552000.0\n",
      "Grad Norm =  15421229000.0\n",
      "Loss =  1865919500.0\n",
      "Grad Norm =  43544240000.0\n",
      "Loss =  1523074600.0\n",
      "Grad Norm =  60230017000.0\n",
      "Loss =  1151379500.0\n",
      "Epoch Reward = 8.0\n",
      "Grad Norm =  743019000.0\n",
      "Loss =  1421305900.0\n",
      "Grad Norm =  1869987200.0\n",
      "Loss =  977760000.0\n",
      "Grad Norm =  44477940000.0\n",
      "Loss =  1486035800.0\n",
      "Grad Norm =  28776667000.0\n",
      "Loss =  1002983200.0\n",
      "Grad Norm =  45282760000.0\n",
      "Loss =  1482854400.0\n",
      "Grad Norm =  67551720000.0\n",
      "Loss =  2354924000.0\n",
      "Grad Norm =  27615515000.0\n",
      "Loss =  998488100.0\n",
      "Grad Norm =  46972387000.0\n",
      "Loss =  1063719940.0\n",
      "Grad Norm =  15078159000.0\n",
      "Loss =  1802101000.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  22671892000.0\n",
      "Loss =  1405870300.0\n",
      "Grad Norm =  14261157000.0\n",
      "Loss =  1783922000.0\n",
      "Grad Norm =  11743560000.0\n",
      "Loss =  954327500.0\n",
      "Grad Norm =  23803836000.0\n",
      "Loss =  1394724500.0\n",
      "Grad Norm =  112263905000.0\n",
      "Loss =  3239688700.0\n",
      "Grad Norm =  15122267000.0\n",
      "Loss =  951756400.0\n",
      "Grad Norm =  32229446000.0\n",
      "Loss =  985546100.0\n",
      "Grad Norm =  35674993000.0\n",
      "Loss =  2493814300.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  63561093000.0\n",
      "Loss =  1134428000.0\n",
      "Grad Norm =  77804530000.0\n",
      "Loss =  781212900.0\n",
      "Grad Norm =  2927271400.0\n",
      "Loss =  1727826200.0\n",
      "Grad Norm =  1031136600.0\n",
      "Loss =  1338698100.0\n",
      "Grad Norm =  42336408000.0\n",
      "Loss =  1792044700.0\n",
      "Grad Norm =  50386220000.0\n",
      "Loss =  1816832300.0\n",
      "Grad Norm =  16444365000.0\n",
      "Loss =  485136500.0\n",
      "Grad Norm =  39490030000.0\n",
      "Loss =  1773446700.0\n",
      "Grad Norm =  25816906000.0\n",
      "Loss =  1734400500.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  29683130000.0\n",
      "Loss =  2695471000.0\n",
      "Grad Norm =  82882520000.0\n",
      "Loss =  1299283600.0\n",
      "Grad Norm =  88825690000.0\n",
      "Loss =  1376559700.0\n",
      "Grad Norm =  17194453000.0\n",
      "Loss =  2893793500.0\n",
      "Grad Norm =  57587044000.0\n",
      "Loss =  1485418200.0\n",
      "Grad Norm =  10295609000.0\n",
      "Loss =  2328361000.0\n",
      "Grad Norm =  60243530000.0\n",
      "Loss =  639466600.0\n",
      "Grad Norm =  4489921000.0\n",
      "Loss =  1290470700.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  59266343000.0\n",
      "Loss =  1419584500.0\n",
      "Grad Norm =  122230240000.0\n",
      "Loss =  2866564400.0\n",
      "Grad Norm =  45892157000.0\n",
      "Loss =  1739527700.0\n",
      "Grad Norm =  30046450000.0\n",
      "Loss =  925900400.0\n",
      "Grad Norm =  1730920200.0\n",
      "Loss =  1982405400.0\n",
      "Grad Norm =  52915814000.0\n",
      "Loss =  1431370900.0\n",
      "Grad Norm =  9767888000.0\n",
      "Loss =  2282924300.0\n",
      "Grad Norm =  75339720000.0\n",
      "Loss =  1206087200.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  29136546000.0\n",
      "Loss =  1303968400.0\n",
      "Grad Norm =  3474117400.0\n",
      "Loss =  1258831500.0\n",
      "Grad Norm =  42091740000.0\n",
      "Loss =  1694092500.0\n",
      "Grad Norm =  94271275000.0\n",
      "Loss =  2618030300.0\n",
      "Grad Norm =  41666298000.0\n",
      "Loss =  1689634300.0\n",
      "Grad Norm =  125132130.0\n",
      "Loss =  1252432600.0\n",
      "Grad Norm =  1534283000.0\n",
      "Loss =  1609406300.0\n",
      "Grad Norm =  16789035000.0\n",
      "Loss =  1620556500.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  3091728000.0\n",
      "Loss =  2225719800.0\n",
      "Grad Norm =  72451965000.0\n",
      "Loss =  1164486700.0\n",
      "Grad Norm =  65331106000.0\n",
      "Loss =  1096151000.0\n",
      "Grad Norm =  12934373000.0\n",
      "Loss =  1594623700.0\n",
      "Grad Norm =  28583336000.0\n",
      "Loss =  1948088600.0\n",
      "Grad Norm =  63457900000.0\n",
      "Loss =  2387254500.0\n",
      "Grad Norm =  61832114000.0\n",
      "Loss =  2376510000.0\n",
      "Epoch Reward = 8.0\n",
      "Grad Norm =  32241502000.0\n",
      "Loss =  896190200.0\n",
      "Grad Norm =  48630407000.0\n",
      "Loss =  554694140.0\n",
      "Grad Norm =  3531079000.0\n",
      "Loss =  1220370700.0\n",
      "Grad Norm =  8581173000.0\n",
      "Loss =  1221861600.0\n",
      "Grad Norm =  155501680.0\n",
      "Loss =  839541000.0\n",
      "Grad Norm =  26272530000.0\n",
      "Loss =  1246856000.0\n",
      "Grad Norm =  8867322000.0\n",
      "Loss =  841799000.0\n",
      "Grad Norm =  24579142000.0\n",
      "Loss =  1241476100.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  8190098400.0\n",
      "Loss =  839223500.0\n",
      "Grad Norm =  39410010000.0\n",
      "Loss =  1952761900.0\n",
      "Grad Norm =  42283635000.0\n",
      "Loss =  2257360100.0\n",
      "Grad Norm =  27855910000.0\n",
      "Loss =  1248929300.0\n",
      "Grad Norm =  69641980000.0\n",
      "Loss =  703614340.0\n",
      "Grad Norm =  17883181000.0\n",
      "Loss =  1565392600.0\n",
      "Grad Norm =  9525093000.0\n",
      "Loss =  1549811200.0\n",
      "Grad Norm =  554187600.0\n",
      "Loss =  1542618200.0\n",
      "Grad Norm =  39768613000.0\n",
      "Loss =  505567420.0\n",
      "Epoch Reward = 10.0\n",
      "Grad Norm =  21257406000.0\n",
      "Loss =  19481438.0\n",
      "Grad Norm =  112789135000.0\n",
      "Loss =  2663965400.0\n",
      "Grad Norm =  28404357000.0\n",
      "Loss =  858121340.0\n",
      "Grad Norm =  10873256000.0\n",
      "Loss =  828923650.0\n",
      "Grad Norm =  10570406000.0\n",
      "Loss =  1198685700.0\n",
      "Grad Norm =  26297817000.0\n",
      "Loss =  1884727700.0\n",
      "Grad Norm =  27069600000.0\n",
      "Loss =  1228845300.0\n",
      "Grad Norm =  6158295600.0\n",
      "Loss =  1846208900.0\n",
      "Epoch Reward = 9.0\n",
      "Grad Norm =  60835510000.0\n",
      "Loss =  1038403200.0\n",
      "Grad Norm =  65960450000.0\n",
      "Loss =  669731200.0\n",
      "Grad Norm =  6064576000.0\n",
      "Loss =  1522213100.0\n",
      "Grad Norm =  31052534000.0\n",
      "Loss =  1565167900.0\n",
      "Grad Norm =  99115770000.0\n",
      "Loss =  2815657200.0\n",
      "Grad Norm =  33371620000.0\n",
      "Loss =  1570984000.0\n",
      "Grad Norm =  29797915000.0\n",
      "Loss =  1873787600.0\n",
      "Grad Norm =  9482548000.0\n",
      "Loss =  1520465700.0\n",
      "Epoch Reward = 9.0\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "metrics = None\n",
    "logged_s = []\n",
    "for ep in range(100):\n",
    "    s = env.reset()\n",
    "    # pi.epsilon = max(0.01, pi.epsilon * 0.95)\n",
    "    # env.record_metrics({'EpsilonGreedy/epsilon': pi.epsilon})\n",
    "    epoch_r = 0.\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        a = pi(s)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "        logged_s.append(s_next)\n",
    "        epoch_r += r\n",
    "        # extend last reward as asymptotic best-case return\n",
    "        if t == env.spec.max_episode_steps - 1:\n",
    "            assert done\n",
    "            r = 1 / (1 - tracer.gamma)  # gamma + gamma^2 + gamma^3 + ... = 1 / (1 - gamma)\n",
    "\n",
    "        # trace rewards and add transition to replay buffer\n",
    "        tracer.add(s, a, r, done)\n",
    "        while tracer:\n",
    "            buffer.add(tracer.pop())\n",
    "\n",
    "        # learn\n",
    "        if len(buffer) >= 100:\n",
    "            transition_batch = buffer.sample(batch_size=32)\n",
    "            metrics = qlearning.update(transition_batch)\n",
    "            env.record_metrics(metrics)\n",
    "\n",
    "        # sync target network\n",
    "        q_targ.soft_update(q, tau=0.01)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        if metrics is not None:\n",
    "            print(\"Grad Norm = \", metrics['QLearning/grads_norm'])\n",
    "            print(\"Loss = \", metrics['QLearning/loss'])\n",
    "        s = s_next\n",
    "    \n",
    "    print(\"Epoch Reward =\", epoch_r)\n",
    "    # early stopping\n",
    "    if env.avg_G > env.spec.reward_threshold:\n",
    "        break\n",
    "\n",
    "\n",
    "# run env one more time to render\n",
    "# coax.utils.generate_gif(env, policy=pi, filepath=f\"./data/{name}.gif\", duration=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e8e61a7-31ef-40c2-90e7-f8509ca0e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = jnp.stack(logged_s, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90cc2b40-23ac-4a87-82fb-e29a1fcb4d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.05873581, 0.5281131 , 0.07975698, 0.8357381 ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e449a0f-833f-4daa-a743-a0502a1bd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_batch = buffer.sample(batch_size=32)\n",
    "metrics = qlearning.update(transition_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
