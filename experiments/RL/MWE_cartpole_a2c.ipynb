{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coax\n",
    "import gym\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from coax.value_losses import mse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the name of this script\n",
    "name = 'a2c'\n",
    "\n",
    "# the cart-pole MDP\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make(\"rpp_gym:InclinedCartpole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"TEST\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = coax.wrappers.TrainMonitor(env, name=name, tensorboard_dir=f\"./data/tensorboard/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.spec.max_episode_steps = 200\n",
    "# env.spec.reward_threshold = 195.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|root|INFO] Initing EMLP (Haiku)\n",
      "[a2c|root|INFO] Linear W components:400 rep:96P+48P⊗V+20P⊗V²+8P⊗V³+4P⊗V⁴\n",
      "[a2c|root|INFO] P cache miss\n",
      "[a2c|root|INFO] Solving basis for P, for G=Z(2)\n",
      "[a2c|root|INFO] P⊗V cache miss\n",
      "[a2c|root|INFO] Solving basis for P⊗V, for G=Z(2)\n",
      "[a2c|root|INFO] P⊗V² cache miss\n",
      "[a2c|root|INFO] Solving basis for P⊗V², for G=Z(2)\n",
      "[a2c|root|INFO] P⊗V³ cache miss\n",
      "[a2c|root|INFO] Solving basis for P⊗V³, for G=Z(2)\n",
      "[a2c|root|INFO] P⊗V⁴ cache miss\n",
      "[a2c|root|INFO] Solving basis for P⊗V⁴, for G=Z(2)\n",
      "[a2c|root|INFO] V cache miss\n",
      "[a2c|root|INFO] Solving basis for V, for G=Z(2)\n",
      "[a2c|root|INFO] V² cache miss\n",
      "[a2c|root|INFO] Solving basis for V², for G=Z(2)\n",
      "[a2c|root|INFO] V³ cache miss\n",
      "[a2c|root|INFO] Solving basis for V³, for G=Z(2)\n",
      "[a2c|root|INFO] V⁴ cache miss\n",
      "[a2c|root|INFO] Solving basis for V⁴, for G=Z(2)\n",
      "[a2c|root|INFO] Linear W components:10000 rep:576V⁰+576V+384V²+216V³+121V⁴+44V⁵+14V⁶+4V⁷+V⁸\n",
      "[a2c|root|INFO] V⁵ cache miss\n",
      "[a2c|root|INFO] Solving basis for V⁵, for G=Z(2)\n",
      "[a2c|root|INFO] V⁶ cache miss\n",
      "[a2c|root|INFO] Solving basis for V⁶, for G=Z(2)\n",
      "[a2c|root|INFO] V⁷ cache miss\n",
      "[a2c|root|INFO] Solving basis for V⁷, for G=Z(2)\n",
      "[a2c|root|INFO] V⁸ cache miss\n",
      "[a2c|root|INFO] Solving basis for V⁸, for G=Z(2)\n",
      "[a2c|root|INFO] Linear W components:200 rep:24V+12V²+5V³+2V⁴+V⁵\n",
      "[a2c|root|INFO] Initing EMLP (Haiku)\n",
      "[a2c|root|INFO] Linear W components:400 rep:96P+48P⊗V+20P⊗V²+8P⊗V³+4P⊗V⁴\n",
      "[a2c|root|INFO] Linear W components:10000 rep:576V⁰+576V+384V²+216V³+121V⁴+44V⁵+14V⁶+4V⁷+V⁸\n",
      "[a2c|root|INFO] Linear W components:100 rep:24V⁰+12V+5V²+2V³+V⁴\n"
     ]
    }
   ],
   "source": [
    "from emlp import T, Scalar\n",
    "from emlp.groups import SO, S, O, Trivial,Z\n",
    "import emlp.nn.haiku as ehk\n",
    "from emlp.reps import Rep\n",
    "from emlp.nn import gated,gate_indices,uniform_rep\n",
    "from math import prod\n",
    "from representations import PseudoScalar\n",
    "from mixed_emlp_haiku import MixedEMLP\n",
    "\n",
    "## Trivial\n",
    "# group=Trivial(2)\n",
    "# rep_in = T(0)*prod(env.observation_space.shape)\n",
    "# rep_out = T(0)*env.action_space.n#prod(env.action_space.shape)\n",
    "\n",
    "## Reflection\n",
    "group=Z(2)\n",
    "rep_in = PseudoScalar()*prod(env.observation_space.shape)\n",
    "rep_out = T(1)#*env.action_space.n#prod(env.action_space.shape)\n",
    "\n",
    "\n",
    "ch = 250\n",
    "num_layers = 4\n",
    "\n",
    "nn_pi = ehk.EMLP(rep_in,rep_out,group,ch=100,num_layers=2)\n",
    "nn_v = ehk.EMLP(rep_in,T(0),group,ch=100,num_layers=2)\n",
    "\n",
    "# nn_pi = ehk.MLP(rep_in,rep_out(group),group,ch=250,num_layers=num_layers)\n",
    "# nn_v = ehk.MLP(rep_in,T(0),group,ch=ch,num_layers=num_layers)\n",
    "\n",
    "# nn_pi = MixedEMLP(rep_in,rep_out(group),group,ch=250,num_layers=5)\n",
    "# nn_v = MixedEMLP(rep_in,T(0),group,ch=250,num_layers=5)\n",
    "\n",
    "\n",
    "def func_pi(S, is_training):\n",
    "    return {'logits': nn_pi(S)}\n",
    "\n",
    "\n",
    "def func_v(S, is_training):\n",
    "    return nn_v(S).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "# def func_pi(S, is_training):\n",
    "#     logits = hk.Sequential((\n",
    "#         hk.Linear(16), jax.nn.relu,\n",
    "#         hk.Linear(16), jax.nn.relu,\n",
    "#         hk.Linear(16), jax.nn.relu,\n",
    "#         hk.Linear(env.action_space.n, w_init=jnp.zeros)\n",
    "#     ))\n",
    "#     return {'logits': logits(S)}\n",
    "\n",
    "\n",
    "\n",
    "# def func_v(S, is_training):\n",
    "#     value = hk.Sequential((\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(32), jax.nn.relu,\n",
    "#         hk.Linear(1, w_init=jnp.zeros), jnp.ravel\n",
    "#     ))\n",
    "#     return value(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# these optimizers collect batches of grads before applying updates\n",
    "optimizer_v = optax.chain(optax.apply_every(k=32), optax.adam(0.002))\n",
    "optimizer_pi = optax.chain(optax.apply_every(k=32), optax.adam(0.001))\n",
    "\n",
    "# value function and its derived policy\n",
    "v = coax.V(func_v, env)\n",
    "pi = coax.Policy(func_pi, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = v.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 1,\tT: 12,\tG: 11,\tavg_r: 1,\tavg_G: 11,\tt: 11,\tdt: 973.330ms,\tSimpleTD/loss: 0.639,\tVanillaPG/loss: 0.774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 2,\tT: 49,\tG: 36,\tavg_r: 1,\tavg_G: 23.5,\tt: 36,\tdt: 28.061ms,\tSimpleTD/loss: 0.524,\tVanillaPG/loss: 0.714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 3,\tT: 64,\tG: 14,\tavg_r: 1,\tavg_G: 20.3,\tt: 14,\tdt: 28.742ms,\tSimpleTD/loss: 0.396,\tVanillaPG/loss: 0.596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 4,\tT: 80,\tG: 15,\tavg_r: 1,\tavg_G: 19,\tt: 15,\tdt: 28.372ms,\tSimpleTD/loss: 0.437,\tVanillaPG/loss: 0.496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 5,\tT: 97,\tG: 16,\tavg_r: 1,\tavg_G: 18.4,\tt: 16,\tdt: 29.613ms,\tSimpleTD/loss: 0.424,\tVanillaPG/loss: 0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 6,\tT: 111,\tG: 13,\tavg_r: 1,\tavg_G: 17.5,\tt: 13,\tdt: 29.448ms,\tSimpleTD/loss: 1.06,\tVanillaPG/loss: 0.253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 7,\tT: 125,\tG: 13,\tavg_r: 1,\tavg_G: 16.9,\tt: 13,\tdt: 27.702ms,\tSimpleTD/loss: 1.5,\tVanillaPG/loss: 0.146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 8,\tT: 139,\tG: 13,\tavg_r: 1,\tavg_G: 16.4,\tt: 13,\tdt: 29.213ms,\tSimpleTD/loss: 1.27,\tVanillaPG/loss: -0.0088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 9,\tT: 156,\tG: 16,\tavg_r: 1,\tavg_G: 16.3,\tt: 16,\tdt: 29.353ms,\tSimpleTD/loss: 1.28,\tVanillaPG/loss: -0.119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 10,\tT: 178,\tG: 21,\tavg_r: 1,\tavg_G: 16.8,\tt: 21,\tdt: 28.104ms,\tSimpleTD/loss: 0.698,\tVanillaPG/loss: 0.0376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 11,\tT: 195,\tG: 16,\tavg_r: 1,\tavg_G: 16.7,\tt: 16,\tdt: 30.011ms,\tSimpleTD/loss: 0.687,\tVanillaPG/loss: 0.0999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 12,\tT: 215,\tG: 19,\tavg_r: 1,\tavg_G: 16.9,\tt: 19,\tdt: 28.882ms,\tSimpleTD/loss: 0.707,\tVanillaPG/loss: 0.105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 13,\tT: 234,\tG: 18,\tavg_r: 1,\tavg_G: 17.1,\tt: 18,\tdt: 29.239ms,\tSimpleTD/loss: 0.477,\tVanillaPG/loss: 0.0619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 14,\tT: 251,\tG: 16,\tavg_r: 1,\tavg_G: 16.9,\tt: 16,\tdt: 28.887ms,\tSimpleTD/loss: 0.207,\tVanillaPG/loss: 3.56e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 15,\tT: 261,\tG: 9,\tavg_r: 1,\tavg_G: 16.2,\tt: 9,\tdt: 29.963ms,\tSimpleTD/loss: 0.32,\tVanillaPG/loss: -0.278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 16,\tT: 288,\tG: 26,\tavg_r: 1,\tavg_G: 17.1,\tt: 26,\tdt: 27.781ms,\tSimpleTD/loss: 0.293,\tVanillaPG/loss: 0.0144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 17,\tT: 303,\tG: 14,\tavg_r: 1,\tavg_G: 16.8,\tt: 14,\tdt: 28.455ms,\tSimpleTD/loss: 0.523,\tVanillaPG/loss: -0.105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 18,\tT: 318,\tG: 14,\tavg_r: 1,\tavg_G: 16.5,\tt: 14,\tdt: 28.498ms,\tSimpleTD/loss: 0.682,\tVanillaPG/loss: -0.115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 19,\tT: 331,\tG: 12,\tavg_r: 1,\tavg_G: 16.1,\tt: 12,\tdt: 28.060ms,\tSimpleTD/loss: 0.683,\tVanillaPG/loss: -0.044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 20,\tT: 344,\tG: 12,\tavg_r: 1,\tavg_G: 15.7,\tt: 12,\tdt: 30.011ms,\tSimpleTD/loss: 0.901,\tVanillaPG/loss: 0.104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 21,\tT: 364,\tG: 19,\tavg_r: 1,\tavg_G: 16,\tt: 19,\tdt: 28.699ms,\tSimpleTD/loss: 0.384,\tVanillaPG/loss: 0.119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 22,\tT: 422,\tG: 57,\tavg_r: 1,\tavg_G: 20.1,\tt: 57,\tdt: 28.832ms,\tSimpleTD/loss: 1.2,\tVanillaPG/loss: -0.0515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 57.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 23,\tT: 449,\tG: 26,\tavg_r: 1,\tavg_G: 20.7,\tt: 26,\tdt: 28.369ms,\tSimpleTD/loss: 0.417,\tVanillaPG/loss: -0.446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 24,\tT: 489,\tG: 39,\tavg_r: 1,\tavg_G: 22.5,\tt: 39,\tdt: 28.605ms,\tSimpleTD/loss: 0.308,\tVanillaPG/loss: 0.331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 25,\tT: 517,\tG: 27,\tavg_r: 1,\tavg_G: 23,\tt: 27,\tdt: 29.525ms,\tSimpleTD/loss: 0.554,\tVanillaPG/loss: 0.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 27.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 26,\tT: 535,\tG: 17,\tavg_r: 1,\tavg_G: 22.4,\tt: 17,\tdt: 28.380ms,\tSimpleTD/loss: 0.972,\tVanillaPG/loss: -0.231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 27,\tT: 578,\tG: 42,\tavg_r: 1,\tavg_G: 24.3,\tt: 42,\tdt: 28.712ms,\tSimpleTD/loss: 0.639,\tVanillaPG/loss: 0.0768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 28,\tT: 636,\tG: 57,\tavg_r: 1,\tavg_G: 27.6,\tt: 57,\tdt: 27.949ms,\tSimpleTD/loss: 0.494,\tVanillaPG/loss: -0.141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 57.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 29,\tT: 669,\tG: 32,\tavg_r: 1,\tavg_G: 28,\tt: 32,\tdt: 27.774ms,\tSimpleTD/loss: 0.649,\tVanillaPG/loss: -0.241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 32.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 30,\tT: 736,\tG: 66,\tavg_r: 1,\tavg_G: 31.8,\tt: 66,\tdt: 27.469ms,\tSimpleTD/loss: 0.602,\tVanillaPG/loss: -0.0115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 66.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 31,\tT: 822,\tG: 85,\tavg_r: 1,\tavg_G: 37.2,\tt: 85,\tdt: 27.029ms,\tSimpleTD/loss: 0.276,\tVanillaPG/loss: -0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 85.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 32,\tT: 865,\tG: 42,\tavg_r: 1,\tavg_G: 37.6,\tt: 42,\tdt: 28.094ms,\tSimpleTD/loss: 0.526,\tVanillaPG/loss: 0.0126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 33,\tT: 933,\tG: 67,\tavg_r: 1,\tavg_G: 40.6,\tt: 67,\tdt: 28.297ms,\tSimpleTD/loss: 0.329,\tVanillaPG/loss: -0.0227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 67.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 34,\tT: 1,038,\tG: 104,\tavg_r: 1,\tavg_G: 46.9,\tt: 104,\tdt: 28.809ms,\tSimpleTD/loss: 0.256,\tVanillaPG/loss: -0.0235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 104.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 35,\tT: 1,151,\tG: 112,\tavg_r: 1,\tavg_G: 53.4,\tt: 112,\tdt: 28.488ms,\tSimpleTD/loss: 0.15,\tVanillaPG/loss: -0.00529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 112.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 36,\tT: 1,213,\tG: 61,\tavg_r: 1,\tavg_G: 54.2,\tt: 61,\tdt: 27.981ms,\tSimpleTD/loss: 0.0206,\tVanillaPG/loss: 0.00981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 61.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 37,\tT: 1,300,\tG: 86,\tavg_r: 1,\tavg_G: 57.4,\tt: 86,\tdt: 27.979ms,\tSimpleTD/loss: 0.0338,\tVanillaPG/loss: -0.0392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 38,\tT: 1,412,\tG: 111,\tavg_r: 1,\tavg_G: 62.7,\tt: 111,\tdt: 27.264ms,\tSimpleTD/loss: 0.0806,\tVanillaPG/loss: -0.0331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 111.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 39,\tT: 1,479,\tG: 66,\tavg_r: 1,\tavg_G: 63.1,\tt: 66,\tdt: 27.973ms,\tSimpleTD/loss: 0.247,\tVanillaPG/loss: -0.0591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 66.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 40,\tT: 1,680,\tG: 200,\tavg_r: 1,\tavg_G: 76.8,\tt: 200,\tdt: 28.862ms,\tSimpleTD/loss: 0.0152,\tVanillaPG/loss: 0.00642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 41,\tT: 1,783,\tG: 102,\tavg_r: 1,\tavg_G: 79.3,\tt: 102,\tdt: 27.854ms,\tSimpleTD/loss: 0.303,\tVanillaPG/loss: -0.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 102.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 42,\tT: 1,971,\tG: 187,\tavg_r: 1,\tavg_G: 90,\tt: 187,\tdt: 28.441ms,\tSimpleTD/loss: 0.241,\tVanillaPG/loss: 0.0122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 187.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 43,\tT: 2,172,\tG: 200,\tavg_r: 1,\tavg_G: 101,\tt: 200,\tdt: 28.684ms,\tSimpleTD/loss: 0.0364,\tVanillaPG/loss: 0.0135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 44,\tT: 2,373,\tG: 200,\tavg_r: 1,\tavg_G: 111,\tt: 200,\tdt: 28.467ms,\tSimpleTD/loss: 0.00417,\tVanillaPG/loss: -0.0177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 45,\tT: 2,537,\tG: 163,\tavg_r: 1,\tavg_G: 116,\tt: 163,\tdt: 28.106ms,\tSimpleTD/loss: 0.118,\tVanillaPG/loss: 0.000405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 163.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 46,\tT: 2,738,\tG: 200,\tavg_r: 1,\tavg_G: 125,\tt: 200,\tdt: 28.081ms,\tSimpleTD/loss: 0.00279,\tVanillaPG/loss: -0.00583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 47,\tT: 2,939,\tG: 200,\tavg_r: 1,\tavg_G: 132,\tt: 200,\tdt: 28.142ms,\tSimpleTD/loss: 0.000975,\tVanillaPG/loss: -0.00822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 48,\tT: 3,139,\tG: 199,\tavg_r: 1,\tavg_G: 139,\tt: 199,\tdt: 28.249ms,\tSimpleTD/loss: 0.104,\tVanillaPG/loss: 0.00231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 199.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 49,\tT: 3,305,\tG: 165,\tavg_r: 1,\tavg_G: 141,\tt: 165,\tdt: 27.649ms,\tSimpleTD/loss: 0.0792,\tVanillaPG/loss: -0.0072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 165.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 50,\tT: 3,395,\tG: 89,\tavg_r: 1,\tavg_G: 136,\tt: 89,\tdt: 27.760ms,\tSimpleTD/loss: 0.209,\tVanillaPG/loss: -0.00927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 89.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 51,\tT: 3,514,\tG: 118,\tavg_r: 1,\tavg_G: 134,\tt: 118,\tdt: 27.660ms,\tSimpleTD/loss: 0.115,\tVanillaPG/loss: 0.000759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 118.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 52,\tT: 3,682,\tG: 167,\tavg_r: 1,\tavg_G: 138,\tt: 167,\tdt: 28.016ms,\tSimpleTD/loss: 0.023,\tVanillaPG/loss: 0.00692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 167.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 53,\tT: 3,876,\tG: 193,\tavg_r: 1,\tavg_G: 143,\tt: 193,\tdt: 27.365ms,\tSimpleTD/loss: 0.0392,\tVanillaPG/loss: -0.0227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 193.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 54,\tT: 4,077,\tG: 200,\tavg_r: 1,\tavg_G: 149,\tt: 200,\tdt: 27.710ms,\tSimpleTD/loss: 0.0191,\tVanillaPG/loss: -0.000488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 55,\tT: 4,278,\tG: 200,\tavg_r: 1,\tavg_G: 154,\tt: 200,\tdt: 28.371ms,\tSimpleTD/loss: 0.0211,\tVanillaPG/loss: -0.00848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 56,\tT: 4,479,\tG: 200,\tavg_r: 1,\tavg_G: 159,\tt: 200,\tdt: 27.865ms,\tSimpleTD/loss: 0.00118,\tVanillaPG/loss: -0.00136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 57,\tT: 4,664,\tG: 184,\tavg_r: 1,\tavg_G: 161,\tt: 184,\tdt: 28.473ms,\tSimpleTD/loss: 0.164,\tVanillaPG/loss: -0.00382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 184.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 58,\tT: 4,865,\tG: 200,\tavg_r: 1,\tavg_G: 165,\tt: 200,\tdt: 28.678ms,\tSimpleTD/loss: 0.0053,\tVanillaPG/loss: 0.000925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 59,\tT: 5,066,\tG: 200,\tavg_r: 1,\tavg_G: 168,\tt: 200,\tdt: 28.865ms,\tSimpleTD/loss: 0.00123,\tVanillaPG/loss: -0.00093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 60,\tT: 5,267,\tG: 200,\tavg_r: 1,\tavg_G: 172,\tt: 200,\tdt: 27.926ms,\tSimpleTD/loss: 0.00309,\tVanillaPG/loss: -0.00117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 61,\tT: 5,468,\tG: 200,\tavg_r: 1,\tavg_G: 174,\tt: 200,\tdt: 28.093ms,\tSimpleTD/loss: 0.00349,\tVanillaPG/loss: -0.00274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 62,\tT: 5,669,\tG: 200,\tavg_r: 1,\tavg_G: 177,\tt: 200,\tdt: 27.876ms,\tSimpleTD/loss: 0.0028,\tVanillaPG/loss: 0.000799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 63,\tT: 5,870,\tG: 200,\tavg_r: 1,\tavg_G: 179,\tt: 200,\tdt: 28.085ms,\tSimpleTD/loss: 0.00199,\tVanillaPG/loss: -0.00128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 64,\tT: 6,071,\tG: 200,\tavg_r: 1,\tavg_G: 181,\tt: 200,\tdt: 27.741ms,\tSimpleTD/loss: 0.00043,\tVanillaPG/loss: -0.00124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 65,\tT: 6,272,\tG: 200,\tavg_r: 1,\tavg_G: 183,\tt: 200,\tdt: 27.554ms,\tSimpleTD/loss: 0.00128,\tVanillaPG/loss: -0.000935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 66,\tT: 6,473,\tG: 200,\tavg_r: 1,\tavg_G: 185,\tt: 200,\tdt: 27.990ms,\tSimpleTD/loss: 0.000233,\tVanillaPG/loss: -0.000825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 67,\tT: 6,674,\tG: 200,\tavg_r: 1,\tavg_G: 186,\tt: 200,\tdt: 28.469ms,\tSimpleTD/loss: 9.96e-05,\tVanillaPG/loss: -6.93e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 68,\tT: 6,875,\tG: 200,\tavg_r: 1,\tavg_G: 188,\tt: 200,\tdt: 28.454ms,\tSimpleTD/loss: 0.000196,\tVanillaPG/loss: -0.000481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 69,\tT: 7,076,\tG: 200,\tavg_r: 1,\tavg_G: 189,\tt: 200,\tdt: 28.063ms,\tSimpleTD/loss: 0.00075,\tVanillaPG/loss: 0.00138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 70,\tT: 7,277,\tG: 200,\tavg_r: 1,\tavg_G: 190,\tt: 200,\tdt: 28.350ms,\tSimpleTD/loss: 0.00384,\tVanillaPG/loss: 0.00114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 71,\tT: 7,478,\tG: 200,\tavg_r: 1,\tavg_G: 191,\tt: 200,\tdt: 27.695ms,\tSimpleTD/loss: 0.00206,\tVanillaPG/loss: -0.00119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 72,\tT: 7,679,\tG: 200,\tavg_r: 1,\tavg_G: 192,\tt: 200,\tdt: 28.361ms,\tSimpleTD/loss: 0.000758,\tVanillaPG/loss: -0.000531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 73,\tT: 7,880,\tG: 200,\tavg_r: 1,\tavg_G: 193,\tt: 200,\tdt: 27.694ms,\tSimpleTD/loss: 0.00031,\tVanillaPG/loss: -0.00219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 74,\tT: 8,081,\tG: 200,\tavg_r: 1,\tavg_G: 194,\tt: 200,\tdt: 27.238ms,\tSimpleTD/loss: 0.000842,\tVanillaPG/loss: -0.000858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 75,\tT: 8,282,\tG: 200,\tavg_r: 1,\tavg_G: 194,\tt: 200,\tdt: 27.388ms,\tSimpleTD/loss: 0.00173,\tVanillaPG/loss: 7.55e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|TrainMonitor|INFO] ep: 76,\tT: 8,483,\tG: 200,\tavg_r: 1,\tavg_G: 195,\tt: 200,\tdt: 27.632ms,\tSimpleTD/loss: 0.00115,\tVanillaPG/loss: -0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch reward 209.0\n",
      "Epoch reward 209.0\n"
     ]
    }
   ],
   "source": [
    "# experience tracer\n",
    "tracer = coax.reward_tracing.NStep(n=1, gamma=0.9)\n",
    "\n",
    "# updaters\n",
    "vanilla_pg = coax.policy_objectives.VanillaPG(pi, optimizer=optimizer_pi)\n",
    "simple_td = coax.td_learning.SimpleTD(v, loss_function=mse, optimizer=optimizer_v)\n",
    "\n",
    "epoch_rewards = []\n",
    "\n",
    "# train\n",
    "for ep in range(1000):\n",
    "    s = env.reset()\n",
    "    er = 0\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        a = pi(s)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "        \n",
    "        if done and (t == env.spec.max_episode_steps - 1):\n",
    "            r = 1 / (1 - tracer.gamma)\n",
    "        er+=r\n",
    "        tracer.add(s, a, r, done)\n",
    "        while tracer:\n",
    "            transition_batch = tracer.pop()\n",
    "            metrics_v, td_error = simple_td.update(transition_batch, return_td_error=True)\n",
    "            metrics_pi = vanilla_pg.update(transition_batch, td_error)\n",
    "            env.record_metrics(metrics_v)\n",
    "            env.record_metrics(metrics_pi)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        s = s_next\n",
    "    \n",
    "    print(\"Epoch reward\",er)\n",
    "    epoch_rewards.append(er)\n",
    "    # early stopping\n",
    "    if env.avg_G > env.spec.reward_threshold:\n",
    "        break\n",
    "\n",
    "\n",
    "# run env one more time to render\n",
    "#coax.utils.generate_gif(env, policy=pi, filepath=f\"./data/{name}.gif\", duration=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coax.utils.dump(pi.params, \"./emlp_pi_params.lz4\")\n",
    "# coax.utils.dump(v.params, \"./emlp_v_params.lz4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
